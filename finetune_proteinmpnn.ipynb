{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LBDillon/Decoding_Bias/blob/main/finetune_proteinmpnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nbB2ImPBhCHK"
      },
      "source": [
        "# Fine-Tuning ProteinMPNN for Alkaliphilic Proteins\n",
        "\n",
        "This notebook fine-tunes ProteinMPNN on alkaliphilic protein structures to shift its\n",
        "sequence design preferences toward alkaline-adapted biophysical properties.\n",
        "\n",
        "## Workflow\n",
        "1. **Convert** alkaliphile dataset from custom format to standard ProteinMPNN training format\n",
        "2. **Fine-tune** from vanilla ProteinMPNN weights using the standard training pipeline\n",
        "3. **Evaluate** training convergence and pH feature shifts in designed sequences\n",
        "\n",
        "## Requirements\n",
        "- Google Colab with GPU runtime\n",
        "- Upload `dataset_alkaliphile/` directory to Google Drive\n",
        "- PyTorch, numpy, biopython"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3v154FwXhCHO",
        "outputId": "9fae2377-d4c3-49d1-deb2-af63887b8293",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Device: cuda:0\n",
            "PyTorch: 2.9.0+cu128\n"
          ]
        }
      ],
      "source": [
        "#@title Cell 1: Setup & Dependencies\n",
        "import os, sys, subprocess\n",
        "\n",
        "# Mount Google Drive (needed when running via Colab extension in VS Code)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Clone ProteinMPNN if not present\n",
        "if not os.path.exists('ProteinMPNN'):\n",
        "    subprocess.run(['git', 'clone', 'https://github.com/dauparas/ProteinMPNN.git'], check=True)\n",
        "\n",
        "sys.path.insert(0, 'ProteinMPNN/training')\n",
        "\n",
        "# Install biopython if not available\n",
        "try:\n",
        "    from Bio.PDB import PDBParser\n",
        "except ImportError:\n",
        "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'biopython'], check=True)\n",
        "    from Bio.PDB import PDBParser\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import csv\n",
        "import shutil\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "from collections import defaultdict\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Device: {device}')\n",
        "print(f'PyTorch: {torch.__version__}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J7Hq5dzwhCHP",
        "outputId": "c9800231-e8f7-44a4-9dfc-bff27846e267",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset found: /content/drive/MyDrive/dataset_alkaliphile\n",
            "Files: ['AF-Q09127-model.pdb', 'AF-Q6A6L4-model.pdb', 'AF-Q59TE0-model.pdb', 'AF-P55935-model.pdb', 'AF-P66352-model.pdb', 'AF-Q966C6-model.pdb', 'AF-P56633-model.pdb', 'AF-P26785-model.pdb', 'AF-Q9V3G1-model.pdb', 'AF-Q9NZM5-model.pdb']...\n",
            "Configuration set.\n"
          ]
        }
      ],
      "source": [
        "#@title Cell 2: Configuration — EDIT PATHS HERE\n",
        "\n",
        "# Path to the alkaliphile dataset on Google Drive\n",
        "# Upload dataset_alkaliphile/ to your Drive, then set the path below\n",
        "DATASET_DIR = '/content/drive/MyDrive/dataset_alkaliphile'  # <-- adjust to your Drive path\n",
        "\n",
        "# Where to write the converted standard-format training data (on Colab VM for speed)\n",
        "CONVERTED_DIR = \"/content/proteinmpnn_data_backbone\"\n",
        "\n",
        "# Pre-trained vanilla ProteinMPNN weights\n",
        "PRETRAINED_WEIGHTS = 'ProteinMPNN/vanilla_model_weights/v_48_002.pt'\n",
        "\n",
        "# Fine-tuning output directory\n",
        "OUTPUT_DIR = '/content/finetune_output'\n",
        "\n",
        "# Training hyperparameters\n",
        "NUM_EPOCHS = 30\n",
        "BATCH_SIZE = 2000         # residues per batch (not proteins)\n",
        "HIDDEN_DIM = 128\n",
        "NUM_ENCODER_LAYERS = 3\n",
        "NUM_NEIGHBORS = 48\n",
        "DROPOUT = 0.1\n",
        "BACKBONE_NOISE = 0.2\n",
        "SAVE_EVERY_N_EPOCHS = 5\n",
        "RELOAD_DATA_EVERY_N = 1\n",
        "MAX_PROTEIN_LENGTH = 2000\n",
        "\n",
        "# Verify the dataset path exists\n",
        "assert os.path.exists(DATASET_DIR), f'Dataset not found at {DATASET_DIR} \\u2014 upload to Drive and update the path'\n",
        "print(f'Dataset found: {DATASET_DIR}')\n",
        "print(f'Files: {os.listdir(DATASET_DIR)[:10]}...')\n",
        "print('Configuration set.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vqsotVOBUJlB",
        "outputId": "fdcb531b-e28e-4d18-91c1-35e3cf477dfa",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/training_override.py\n"
          ]
        }
      ],
      "source": [
        "\n",
        "#@title Cell 3: Override trainign script to load data in-process (no ProcessPoolExecutor) for better performance on Colab\n",
        "\n",
        "%%writefile /content/training_override.py\n",
        "import os, time\n",
        "import numpy as np\n",
        "import torch\n",
        "from utils import worker_init_fn, get_pdbs, loader_pdb, build_training_clusters, PDB_dataset, StructureDataset, StructureLoader\n",
        "from model_utils import featurize, loss_smoothed, loss_nll, get_std_opt, ProteinMPNN\n",
        "\n",
        "def main(args):\n",
        "    scaler = torch.cuda.amp.GradScaler()\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    base_folder = time.strftime(args.path_for_outputs, time.localtime())\n",
        "    if base_folder[-1] != '/':\n",
        "        base_folder += '/'\n",
        "    os.makedirs(base_folder, exist_ok=True)\n",
        "    os.makedirs(base_folder + \"model_weights\", exist_ok=True)\n",
        "\n",
        "    logfile = base_folder + \"log.txt\"\n",
        "    with open(logfile, \"a\") as f:\n",
        "        f.write(\"starting\\n\")\n",
        "\n",
        "    data_path = args.path_for_training_data\n",
        "    params = {\n",
        "        \"LIST\": f\"{data_path}/list.csv\",\n",
        "        \"VAL\": f\"{data_path}/valid_clusters.txt\",\n",
        "        \"TEST\": f\"{data_path}/test_clusters.txt\",\n",
        "        \"DIR\": f\"{data_path}\",\n",
        "        \"DATCUT\": \"2030-Jan-01\",\n",
        "        \"RESCUT\": args.rescut,\n",
        "        \"HOMO\": 0.70,\n",
        "    }\n",
        "\n",
        "    # Keep DataLoader workers controllable (but default 0 is good on Colab)\n",
        "    dataloader_workers = getattr(args, \"dataloader_workers\", 0)\n",
        "    LOAD_PARAM = dict(batch_size=1, shuffle=True, pin_memory=False, num_workers=dataloader_workers)\n",
        "\n",
        "    if args.debug:\n",
        "        args.num_examples_per_epoch = 50\n",
        "        args.max_protein_length = 1000\n",
        "        args.batch_size = 1000\n",
        "\n",
        "    train, valid, test = build_training_clusters(params, args.debug)\n",
        "\n",
        "    train_set = PDB_dataset(list(train.keys()), loader_pdb, train, params)\n",
        "    train_loader = torch.utils.data.DataLoader(train_set, worker_init_fn=worker_init_fn, **LOAD_PARAM)\n",
        "    valid_set = PDB_dataset(list(valid.keys()), loader_pdb, valid, params)\n",
        "    valid_loader = torch.utils.data.DataLoader(valid_set, worker_init_fn=worker_init_fn, **LOAD_PARAM)\n",
        "\n",
        "    model = ProteinMPNN(\n",
        "        node_features=args.hidden_dim,\n",
        "        edge_features=args.hidden_dim,\n",
        "        hidden_dim=args.hidden_dim,\n",
        "        num_encoder_layers=args.num_encoder_layers,\n",
        "        num_decoder_layers=args.num_encoder_layers,\n",
        "        k_neighbors=args.num_neighbors,\n",
        "        dropout=args.dropout,\n",
        "        augment_eps=args.backbone_noise,\n",
        "    ).to(device)\n",
        "\n",
        "    PATH = args.previous_checkpoint\n",
        "    if PATH:\n",
        "        checkpoint = torch.load(PATH, map_location=\"cpu\", weights_only=False)\n",
        "        total_step = checkpoint.get(\"step\", 0)\n",
        "        epoch0 = checkpoint.get(\"epoch\", 0)\n",
        "        model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "    else:\n",
        "        total_step = 0\n",
        "        epoch0 = 0\n",
        "        checkpoint = None\n",
        "\n",
        "    optimizer = get_std_opt(model.parameters(), args.hidden_dim, total_step)\n",
        "    if PATH and checkpoint is not None and (\"optimizer_state_dict\" in checkpoint) and (checkpoint[\"optimizer_state_dict\"] is not None):\n",
        "        optimizer.optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
        "\n",
        "    print(f\"[override] dataloader_workers={dataloader_workers}\")\n",
        "    print(\"[override] Loading get_pdbs in-process (no ProcessPoolExecutor)...\")\n",
        "\n",
        "    # Prefetch initial epoch data in-process (fast for your dataset)\n",
        "    pdb_dict_train = get_pdbs(train_loader, 1, args.max_protein_length, args.num_examples_per_epoch)\n",
        "    pdb_dict_valid = get_pdbs(valid_loader, 1, args.max_protein_length, args.num_examples_per_epoch)\n",
        "    print(f\"[override] got train={len(pdb_dict_train)} valid={len(pdb_dict_valid)}\")\n",
        "\n",
        "    dataset_train = StructureDataset(pdb_dict_train, truncate=None, max_length=args.max_protein_length)\n",
        "    dataset_valid = StructureDataset(pdb_dict_valid, truncate=None, max_length=args.max_protein_length)\n",
        "    loader_train = StructureLoader(dataset_train, batch_size=args.batch_size)\n",
        "    loader_valid = StructureLoader(dataset_valid, batch_size=args.batch_size)\n",
        "\n",
        "    reload_c = 0\n",
        "    for e1 in range(args.num_epochs):\n",
        "        t0 = time.time()\n",
        "        e = epoch0 + e1\n",
        "        model.train()\n",
        "\n",
        "        train_sum = train_weights = train_acc = 0.0\n",
        "\n",
        "        # Reload data periodically (also in-process)\n",
        "        if e % args.reload_data_every_n_epochs == 0:\n",
        "            if reload_c != 0:\n",
        "                pdb_dict_train = get_pdbs(train_loader, 1, args.max_protein_length, args.num_examples_per_epoch)\n",
        "                pdb_dict_valid = get_pdbs(valid_loader, 1, args.max_protein_length, args.num_examples_per_epoch)\n",
        "\n",
        "                dataset_train = StructureDataset(pdb_dict_train, truncate=None, max_length=args.max_protein_length)\n",
        "                dataset_valid = StructureDataset(pdb_dict_valid, truncate=None, max_length=args.max_protein_length)\n",
        "                loader_train = StructureLoader(dataset_train, batch_size=args.batch_size)\n",
        "                loader_valid = StructureLoader(dataset_valid, batch_size=args.batch_size)\n",
        "            reload_c += 1\n",
        "\n",
        "        for _, batch in enumerate(loader_train):\n",
        "            X, S, mask, lengths, chain_M, residue_idx, mask_self, chain_encoding_all = featurize(batch, device)\n",
        "            optimizer.zero_grad()\n",
        "            mask_for_loss = mask * chain_M\n",
        "\n",
        "            if args.mixed_precision:\n",
        "                with torch.cuda.amp.autocast():\n",
        "                    log_probs = model(X, S, mask, chain_M, residue_idx, chain_encoding_all)\n",
        "                    _, loss_av_smoothed = loss_smoothed(S, log_probs, mask_for_loss)\n",
        "                scaler.scale(loss_av_smoothed).backward()\n",
        "                scaler.step(optimizer)\n",
        "                scaler.update()\n",
        "            else:\n",
        "                log_probs = model(X, S, mask, chain_M, residue_idx, chain_encoding_all)\n",
        "                _, loss_av_smoothed = loss_smoothed(S, log_probs, mask_for_loss)\n",
        "                loss_av_smoothed.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "            loss, loss_av, true_false = loss_nll(S, log_probs, mask_for_loss)\n",
        "            train_sum += torch.sum(loss * mask_for_loss).detach().cpu().numpy()\n",
        "            train_acc += torch.sum(true_false * mask_for_loss).detach().cpu().numpy()\n",
        "            train_weights += torch.sum(mask_for_loss).detach().cpu().numpy()\n",
        "            total_step += 1\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            val_sum = val_weights = val_acc = 0.0\n",
        "            for _, batch in enumerate(loader_valid):\n",
        "                X, S, mask, lengths, chain_M, residue_idx, mask_self, chain_encoding_all = featurize(batch, device)\n",
        "                log_probs = model(X, S, mask, chain_M, residue_idx, chain_encoding_all)\n",
        "                mask_for_loss = mask * chain_M\n",
        "                loss, loss_av, true_false = loss_nll(S, log_probs, mask_for_loss)\n",
        "                val_sum += torch.sum(loss * mask_for_loss).detach().cpu().numpy()\n",
        "                val_acc += torch.sum(true_false * mask_for_loss).detach().cpu().numpy()\n",
        "                val_weights += torch.sum(mask_for_loss).detach().cpu().numpy()\n",
        "\n",
        "        train_loss = train_sum / train_weights\n",
        "        val_loss = val_sum / val_weights\n",
        "        line = (\n",
        "            f\"epoch: {e+1}, step: {total_step}, time: {time.time()-t0:.1f}, \"\n",
        "            f\"train: {np.exp(train_loss):.3f}, valid: {np.exp(val_loss):.3f}, \"\n",
        "            f\"train_acc: {train_acc/train_weights:.3f}, valid_acc: {val_acc/val_weights:.3f}\\n\"\n",
        "        )\n",
        "        with open(logfile, \"a\") as f:\n",
        "            f.write(line)\n",
        "        print(line.strip())\n",
        "\n",
        "        torch.save({\n",
        "            \"epoch\": e+1,\n",
        "            \"step\": total_step,\n",
        "            \"model_state_dict\": model.state_dict(),\n",
        "            \"optimizer_state_dict\": optimizer.optimizer.state_dict(),\n",
        "        }, base_folder + \"model_weights/epoch_last.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TbT5cyMIhCHP",
        "outputId": "3a8fafa5-696d-4fa2-d478-cbc0b9110e78",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Source list.csv: 1993 entries\n",
            "Searching for PDBs in: /content/drive/MyDrive/dataset_alkaliphile (dataset root (flattened))\n",
            "Found 1980 AlphaFold PDB files\n",
            "  Converted 200 proteins...\n",
            "  Converted 400 proteins...\n",
            "  Converted 600 proteins...\n",
            "  Converted 800 proteins...\n",
            "  Converted 1000 proteins...\n",
            "  Converted 1200 proteins...\n",
            "  Converted 1400 proteins...\n",
            "  Converted 1600 proteins...\n",
            "  Converted 1800 proteins...\n",
            "Copied valid_clusters.txt (60 clusters)\n",
            "Copied test_clusters.txt (60 clusters)\n",
            "\n",
            "Done: 1980 converted, 0 skipped\n"
          ]
        }
      ],
      "source": [
        "#@title Cell 4: Convert Dataset to Standard ProteinMPNN Format\n",
        "#\n",
        "# Parses AlphaFold PDB files directly to produce the exact .pt format that\n",
        "# ProteinMPNN's loader_pdb() expects. This avoids all format mismatch issues.\n",
        "#\n",
        "# loader_pdb() constructs paths as:\n",
        "#   PREFIX = \"{DIR}/pdb/{pdbid[1:3]}/{pdbid}\"\n",
        "#   meta = torch.load(PREFIX + \".pt\")\n",
        "#   chain = torch.load(PREFIX + \"_A.pt\")\n",
        "#\n",
        "# So files go DIRECTLY in the shard directory (no per-protein subfolder):\n",
        "#   pdb/0a/a0a087wnh4.pt       — metadata dict\n",
        "#   pdb/0a/a0a087wnh4_A.pt     — chain dict with 'seq' + 'xyz' [L, 14, 3]\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Atom name tables — identical to ProteinMPNN's parse_cif_noX.py\n",
        "RES_NAMES_1 = 'ARNDCQEGHILKMFPSTWYV'\n",
        "RES_NAMES = ['ALA','ARG','ASN','ASP','CYS','GLN','GLU','GLY','HIS','ILE',\n",
        "             'LEU','LYS','MET','PHE','PRO','SER','THR','TRP','TYR','VAL']\n",
        "to1letter = {aaa: a for a, aaa in zip(RES_NAMES_1, RES_NAMES)}\n",
        "\n",
        "ATOM_NAMES = [\n",
        "    ('N','CA','C','O','CB'),  # ala\n",
        "    ('N','CA','C','O','CB','CG','CD','NE','CZ','NH1','NH2'),  # arg\n",
        "    ('N','CA','C','O','CB','CG','OD1','ND2'),  # asn\n",
        "    ('N','CA','C','O','CB','CG','OD1','OD2'),  # asp\n",
        "    ('N','CA','C','O','CB','SG'),  # cys\n",
        "    ('N','CA','C','O','CB','CG','CD','OE1','NE2'),  # gln\n",
        "    ('N','CA','C','O','CB','CG','CD','OE1','OE2'),  # glu\n",
        "    ('N','CA','C','O'),  # gly\n",
        "    ('N','CA','C','O','CB','CG','ND1','CD2','CE1','NE2'),  # his\n",
        "    ('N','CA','C','O','CB','CG1','CG2','CD1'),  # ile\n",
        "    ('N','CA','C','O','CB','CG','CD1','CD2'),  # leu\n",
        "    ('N','CA','C','O','CB','CG','CD','CE','NZ'),  # lys\n",
        "    ('N','CA','C','O','CB','CG','SD','CE'),  # met\n",
        "    ('N','CA','C','O','CB','CG','CD1','CD2','CE1','CE2','CZ'),  # phe\n",
        "    ('N','CA','C','O','CB','CG','CD'),  # pro\n",
        "    ('N','CA','C','O','CB','OG'),  # ser\n",
        "    ('N','CA','C','O','CB','OG1','CG2'),  # thr\n",
        "    ('N','CA','C','O','CB','CG','CD1','CD2','CE2','CE3','NE1','CZ2','CZ3','CH2'),  # trp\n",
        "    ('N','CA','C','O','CB','CG','CD1','CD2','CE1','CE2','CZ','OH'),  # tyr\n",
        "    ('N','CA','C','O','CB','CG1','CG2'),  # val\n",
        "]\n",
        "\n",
        "# (resname, atomname) -> atom slot index\n",
        "aa2idx = {(RES_NAMES[i], a): j for i in range(20) for j, a in enumerate(ATOM_NAMES[i])}\n",
        "\n",
        "\n",
        "def parse_af_pdb(pdb_path):\n",
        "    \"\"\"Parse an AlphaFold PDB file into sequence + all-atom coordinates.\n",
        "\n",
        "    Returns:\n",
        "        seq: str, amino acid sequence\n",
        "        xyz: np.ndarray [L, 14, 3], all-atom coords (NaN where atom absent)\n",
        "    \"\"\"\n",
        "    parser = PDBParser(QUIET=True)\n",
        "    struct = parser.get_structure('prot', pdb_path)\n",
        "    model = struct[0]\n",
        "    chain = model['A']\n",
        "    residues = [r for r in chain.get_residues() if r.get_resname() in to1letter]\n",
        "\n",
        "    if len(residues) == 0:\n",
        "        return None, None\n",
        "\n",
        "    L = len(residues)\n",
        "    seq = ''.join([to1letter[r.get_resname()] for r in residues])\n",
        "    xyz = np.full((L, 14, 3), np.nan, dtype=np.float32)\n",
        "\n",
        "    for i, res in enumerate(residues):\n",
        "        resname = res.get_resname()\n",
        "        for atom in res.get_atoms():\n",
        "            key = (resname, atom.get_name())\n",
        "            if key in aa2idx:\n",
        "                xyz[i, aa2idx[key]] = atom.get_vector().get_array()\n",
        "\n",
        "    return seq, xyz\n",
        "def parse_af_pdb_backbone(pdb_path):\n",
        "    \"\"\"\n",
        "    Parse AlphaFold PDB into (seq, xyz_bb) where xyz_bb is [L, 4, 3] for N,CA,C,O.\n",
        "    Returns (None, None) if backbone is incomplete (any NaNs).\n",
        "    \"\"\"\n",
        "    seq, xyz14 = parse_af_pdb(pdb_path)     # existing: [L,14,3] with NaNs for missing atoms\n",
        "    if seq is None:\n",
        "        return None, None\n",
        "\n",
        "    xyz_bb = xyz14[:, :4, :]                # N, CA, C, O\n",
        "    if np.isnan(xyz_bb).any():\n",
        "        return None, None\n",
        "\n",
        "    return seq, xyz_bb\n",
        "\n",
        "def _norm_id(x: str) -> str:\n",
        "    \"\"\"\n",
        "    Normalize identifiers from either list.csv or AF PDB filenames to a canonical uniprot id:\n",
        "    - lowercased\n",
        "    - removes leading 'af-'\n",
        "    - removes trailing '-model'\n",
        "    - removes chain suffix after '_' (e.g. '_A')\n",
        "    \"\"\"\n",
        "    s = str(x).strip().lower()\n",
        "    # drop anything after whitespace\n",
        "    s = s.split()[0]\n",
        "    # drop chain suffix if present\n",
        "    if \"_\" in s:\n",
        "        s = s.split(\"_\")[0]\n",
        "    # strip AF prefix/suffix patterns\n",
        "    if s.startswith(\"af-\"):\n",
        "        s = s[3:]\n",
        "    if s.endswith(\"-model\"):\n",
        "        s = s[:-5]\n",
        "    return s\n",
        "\n",
        "\n",
        "def _uniprot_from_af_filename(p: Path) -> str | None:\n",
        "    \"\"\"\n",
        "    Extract UniProt ID from AlphaFold filename patterns like:\n",
        "      AF-Q09127-model.pdb\n",
        "      AF-A0A087WNH4-model.pdb\n",
        "    Returns normalized id (lowercase) or None if it can't parse.\n",
        "    \"\"\"\n",
        "    stem = p.stem  # e.g. \"AF-Q09127-model\"\n",
        "    parts = stem.split(\"-\")\n",
        "    # Expect [\"AF\", \"<UNIPROT>\", \"model\"] at minimum\n",
        "    if len(parts) >= 3 and parts[0].lower() == \"af\":\n",
        "        return _norm_id(parts[1])\n",
        "    # fallback: try to find something that looks like uniprot token in the middle\n",
        "    if len(parts) >= 2:\n",
        "        return _norm_id(parts[1])\n",
        "    return None\n",
        "\n",
        "\n",
        "def convert_alkaliphile_to_standard(dataset_dir, output_dir):\n",
        "    dataset_dir = Path(dataset_dir)\n",
        "    output_dir = Path(output_dir)\n",
        "    pdb_out = output_dir / \"pdb\"\n",
        "    pdb_out.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # 1) Read list.csv for cluster assignments\n",
        "    df = pd.read_csv(dataset_dir / \"list.csv\")\n",
        "    print(f\"Source list.csv: {len(df)} entries\")\n",
        "\n",
        "    # Build lookup keyed by normalized uniprot id\n",
        "    lookup = {}\n",
        "    for _, row in df.iterrows():\n",
        "        chainid_raw = str(row[\"CHAINID\"]).strip()\n",
        "        key = _norm_id(chainid_raw)\n",
        "        lookup[key] = {\n",
        "            \"cluster\": int(row[\"CLUSTER\"]),\n",
        "            \"split\": str(row[\"SPLIT\"]).strip(),\n",
        "        }\n",
        "\n",
        "    # 2) Find where the PDB files actually live\n",
        "    af_pdb_dir = dataset_dir / \"AF_PDBs\"\n",
        "    if af_pdb_dir.exists() and af_pdb_dir.is_dir():\n",
        "        search_dir = af_pdb_dir\n",
        "        layout = \"AF_PDBs/ subfolder\"\n",
        "    else:\n",
        "        search_dir = dataset_dir\n",
        "        layout = \"dataset root (flattened)\"\n",
        "\n",
        "    pdb_files = sorted(search_dir.glob(\"AF-*-model.pdb\"))\n",
        "    # Optional: if you have other patterns, broaden the glob:\n",
        "    # pdb_files = sorted(search_dir.glob(\"*.pdb\"))\n",
        "\n",
        "    print(f\"Searching for PDBs in: {search_dir} ({layout})\")\n",
        "    print(f\"Found {len(pdb_files)} AlphaFold PDB files\")\n",
        "\n",
        "    if len(pdb_files) == 0:\n",
        "        # Give a helpful diagnostic list\n",
        "        sample = sorted([p.name for p in search_dir.glob(\"*.pdb\")])[:20]\n",
        "        print(\"No files matched 'AF-*-model.pdb'. Here are some .pdb files I can see:\")\n",
        "        print(sample)\n",
        "        return 0\n",
        "\n",
        "    converted = 0\n",
        "    skipped = 0\n",
        "    csv_rows = []\n",
        "\n",
        "    for pdb_path in pdb_files:\n",
        "        uniprot_id = _uniprot_from_af_filename(pdb_path)\n",
        "        if uniprot_id is None:\n",
        "            skipped += 1\n",
        "            continue\n",
        "\n",
        "        if uniprot_id not in lookup:\n",
        "            skipped += 1\n",
        "            continue\n",
        "\n",
        "        info = lookup[uniprot_id]\n",
        "\n",
        "        # Parse PDB -> sequence + BACKBONE coordinates only (no NaNs allowed)\n",
        "        seq, xyz_bb = parse_af_pdb_backbone(str(pdb_path))\n",
        "        if seq is None or len(seq) < 4:\n",
        "            skipped += 1\n",
        "            continue\n",
        "\n",
        "        shard = uniprot_id[1:3]\n",
        "        shard_dir = pdb_out / shard\n",
        "        shard_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        torch.save(\n",
        "        {\"seq\": seq, \"xyz\": torch.tensor(xyz_bb)},   # [L,4,3], finite\n",
        "        shard_dir / f\"{uniprot_id}_A.pt\"\n",
        "    )\n",
        "\n",
        "        torch.save(\n",
        "            {\n",
        "                \"asmb_ids\": [\"1\"],\n",
        "                \"asmb_chains\": [\"A\"],\n",
        "                \"chains\": [\"A\"],\n",
        "                \"tm\": torch.tensor([[[1.0, 1.0, 0.0]]]),\n",
        "                \"asmb_xform0\": torch.eye(4).unsqueeze(0),\n",
        "            },\n",
        "            shard_dir / f\"{uniprot_id}.pt\"\n",
        "        )\n",
        "\n",
        "        csv_rows.append([f\"{uniprot_id}_A\", \"2025-Jun-21\", \"-1.0\", \"A\", str(info[\"cluster\"])])\n",
        "        converted += 1\n",
        "\n",
        "        if converted % 200 == 0:\n",
        "            print(f\"  Converted {converted} proteins...\")\n",
        "\n",
        "    with open(output_dir / \"list.csv\", \"w\", newline=\"\") as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow([\"pdbid_chain\", \"date\", \"resolution\", \"chain_id\", \"cluster_id\"])\n",
        "        writer.writerows(csv_rows)\n",
        "\n",
        "    for fname in [\"valid_clusters.txt\", \"test_clusters.txt\"]:\n",
        "        src = dataset_dir / fname\n",
        "        if src.exists():\n",
        "            shutil.copy2(src, output_dir / fname)\n",
        "            n = sum(1 for line in open(src) if line.strip())\n",
        "            print(f\"Copied {fname} ({n} clusters)\")\n",
        "\n",
        "    print(f\"\\nDone: {converted} converted, {skipped} skipped\")\n",
        "    return converted\n",
        "\n",
        "# Run conversion\n",
        "from pathlib import Path\n",
        "if not (Path(CONVERTED_DIR)/\"list.csv\").exists():\n",
        "    n_converted = convert_alkaliphile_to_standard(DATASET_DIR, CONVERTED_DIR)\n",
        "else:\n",
        "    print(\"Converted dataset already exists; skipping conversion.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tn7XO9djhCHQ",
        "outputId": "133768d0-2473-4c47-e231-940f33028eaa",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train clusters: 479, Valid clusters: 60, Test clusters: 60\n",
            "Train proteins: 1620, Valid proteins: 168, Test proteins: 192\n",
            "\n",
            "--- Testing standard loader_pdb() ---\n",
            "  a0a087wnh4_A: seq_len=25, xyz_shape=torch.Size([25, 4, 3]), OK\n",
            "  a0a087wri3_A: seq_len=283, xyz_shape=torch.Size([283, 4, 3]), OK\n",
            "  a0a0b5a8q2_A: seq_len=115, xyz_shape=torch.Size([115, 4, 3]), OK\n",
            "\n",
            "--- Testing full pipeline (get_pdbs -> featurize) ---\n",
            "get_pdbs returned 10 proteins\n",
            "featurize OK: X=torch.Size([2, 320, 4, 3]), S=torch.Size([2, 320]), mask=torch.Size([2, 320])\n",
            "\n",
            "Full pipeline verification PASSED\n"
          ]
        }
      ],
      "source": [
        "#@title Cell 5: Verify Conversion — Test with Standard ProteinMPNN Loader\n",
        "from utils import loader_pdb, build_training_clusters, get_pdbs, PDB_dataset, StructureDataset, StructureLoader\n",
        "from model_utils import featurize\n",
        "\n",
        "# Set up params the same way training.py does\n",
        "params = {\n",
        "    'LIST': f'{CONVERTED_DIR}/list.csv',\n",
        "    'VAL': f'{CONVERTED_DIR}/valid_clusters.txt',\n",
        "    'TEST': f'{CONVERTED_DIR}/test_clusters.txt',\n",
        "    'DIR': CONVERTED_DIR,\n",
        "    'DATCUT': '2030-Jan-01',\n",
        "    'RESCUT': 3.5,\n",
        "    'HOMO': 0.70,\n",
        "}\n",
        "\n",
        "# Build clusters (this is what training.py does)\n",
        "train, valid, test = build_training_clusters(params, debug=False)\n",
        "print(f'Train clusters: {len(train)}, Valid clusters: {len(valid)}, Test clusters: {len(test)}')\n",
        "\n",
        "# Count total proteins\n",
        "n_train = sum(len(v) for v in train.values())\n",
        "n_valid = sum(len(v) for v in valid.values())\n",
        "n_test = sum(len(v) for v in test.values())\n",
        "print(f'Train proteins: {n_train}, Valid proteins: {n_valid}, Test proteins: {n_test}')\n",
        "\n",
        "# Test loading a few proteins through the standard pipeline\n",
        "print('\\n--- Testing standard loader_pdb() ---')\n",
        "test_items = list(train.values())[:3]\n",
        "for item_list in test_items:\n",
        "    item = item_list[0]  # [pdbid_chain, chain_id]\n",
        "    result = loader_pdb(item, params)\n",
        "    if isinstance(result.get('seq', None), str) and len(result['seq']) > 4:\n",
        "        print(f'  {item[0]}: seq_len={len(result[\"seq\"])}, xyz_shape={result[\"xyz\"].shape}, OK')\n",
        "    elif isinstance(result.get('seq', None), np.ndarray) and len(result['seq']) <= 5:\n",
        "        print(f'  {item[0]}: FAILED (returned dummy)')\n",
        "    else:\n",
        "        print(f'  {item[0]}: seq_type={type(result.get(\"seq\"))}')\n",
        "\n",
        "# Test the full pipeline: loader -> get_pdbs -> StructureDataset -> featurize\n",
        "print('\\n--- Testing full pipeline (get_pdbs -> featurize) ---')\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_set = PDB_dataset(list(train.keys())[:10], loader_pdb, train, params)\n",
        "train_loader = DataLoader(train_set, batch_size=1, shuffle=False, num_workers=0)\n",
        "\n",
        "pdb_dict_list = get_pdbs(train_loader, 1, MAX_PROTEIN_LENGTH, 10)\n",
        "print(f'get_pdbs returned {len(pdb_dict_list)} proteins')\n",
        "\n",
        "if len(pdb_dict_list) > 0:\n",
        "    dataset = StructureDataset(pdb_dict_list, truncate=None, max_length=MAX_PROTEIN_LENGTH)\n",
        "    loader = StructureLoader(dataset, batch_size=BATCH_SIZE)\n",
        "    for batch in loader:\n",
        "        X, S, mask, lengths, chain_M, residue_idx, mask_self, chain_encoding_all = featurize(batch, device)\n",
        "        print(f'featurize OK: X={X.shape}, S={S.shape}, mask={mask.shape}')\n",
        "        break\n",
        "    print('\\nFull pipeline verification PASSED')\n",
        "else:\n",
        "    print('WARNING: get_pdbs returned 0 proteins. Check conversion.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bKo3P8A_hCHQ",
        "outputId": "6c9e8caa-f49c-4982-b21a-dd92e7f41903",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pretrained weights found: ProteinMPNN/vanilla_model_weights/v_48_002.pt\n",
            "Checkpoint keys: ['num_edges', 'noise_level', 'model_state_dict']\n",
            "Num parameters: 1,660,485\n"
          ]
        }
      ],
      "source": [
        "#@title Cell 6: Download Pretrained Weights (if not already present)\n",
        "import urllib.request\n",
        "\n",
        "weights_path = Path(PRETRAINED_WEIGHTS)\n",
        "if not weights_path.exists():\n",
        "    # Download from ProteinMPNN release\n",
        "    url = 'https://github.com/dauparas/ProteinMPNN/raw/main/vanilla_model_weights/v_48_002.pt'\n",
        "    weights_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    print(f'Downloading pretrained weights to {weights_path}...')\n",
        "    urllib.request.urlretrieve(url, weights_path)\n",
        "    print('Done.')\n",
        "else:\n",
        "    print(f'Pretrained weights found: {weights_path}')\n",
        "\n",
        "# Quick check\n",
        "ckpt = torch.load(weights_path, map_location='cpu', weights_only=False)\n",
        "print(f'Checkpoint keys: {list(ckpt.keys())}')\n",
        "print(f'Num parameters: {sum(p.numel() for p in ckpt[\"model_state_dict\"].values()):,}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "orJ9LG6QnBjK",
        "outputId": "a858f712-7d42-4daa-9072-1ea634b34048",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote checkpoint with optimizer state to: /content/ProteinMPNN/vanilla_model_weights/v_48_002_wrapped_withopt.pt\n",
            "Keys: ['step', 'epoch', 'model_state_dict', 'optimizer_state_dict']\n"
          ]
        }
      ],
      "source": [
        "#@title Cell 7: Make Full Checkpoint with Fresh Optimizer State\n",
        "import torch\n",
        "from model_utils import get_std_opt, ProteinMPNN\n",
        "\n",
        "def make_full_checkpoint_with_fresh_optimizer(vanilla_path: str, out_path: str,\n",
        "                                             hidden_dim=128, num_encoder_layers=3,\n",
        "                                             num_neighbors=48, dropout=0.1, backbone_noise=0.2):\n",
        "    vanilla = torch.load(vanilla_path, map_location=\"cpu\", weights_only=False)\n",
        "    if isinstance(vanilla, dict) and \"model_state_dict\" in vanilla:\n",
        "        state = vanilla[\"model_state_dict\"]\n",
        "    else:\n",
        "        state = vanilla\n",
        "\n",
        "    # Build model exactly like training.py\n",
        "    model = ProteinMPNN(\n",
        "        node_features=hidden_dim,\n",
        "        edge_features=hidden_dim,\n",
        "        hidden_dim=hidden_dim,\n",
        "        num_encoder_layers=num_encoder_layers,\n",
        "        num_decoder_layers=num_encoder_layers,\n",
        "        k_neighbors=num_neighbors,\n",
        "        dropout=dropout,\n",
        "        augment_eps=backbone_noise\n",
        "    )\n",
        "    model.load_state_dict(state)\n",
        "\n",
        "    # Create optimizer wrapper exactly like training.py does\n",
        "    total_step = 0\n",
        "    opt = get_std_opt(model.parameters(), hidden_dim, total_step)\n",
        "\n",
        "    ckpt = {\n",
        "        \"step\": 0,\n",
        "        \"epoch\": 0,\n",
        "        \"model_state_dict\": model.state_dict(),\n",
        "        \"optimizer_state_dict\": opt.optimizer.state_dict(),  # NOTE: .optimizer (matches training.py)\n",
        "    }\n",
        "    torch.save(ckpt, out_path)\n",
        "    print(f\"Wrote checkpoint with optimizer state to: {out_path}\")\n",
        "    print(f\"Keys: {list(ckpt.keys())}\")\n",
        "\n",
        "wrapped_ckpt3 = \"/content/ProteinMPNN/vanilla_model_weights/v_48_002_wrapped_withopt.pt\"\n",
        "make_full_checkpoint_with_fresh_optimizer(\n",
        "    PRETRAINED_WEIGHTS,\n",
        "    wrapped_ckpt3,\n",
        "    hidden_dim=HIDDEN_DIM,\n",
        "    num_encoder_layers=NUM_ENCODER_LAYERS,\n",
        "    num_neighbors=NUM_NEIGHBORS,\n",
        "    dropout=DROPOUT,\n",
        "    backbone_noise=BACKBONE_NOISE\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mwjo5mXihCHR",
        "outputId": "7d536d17-f750-43d0-8538-2b8f700e6ff3",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Starting Fine-Tuning ===\n",
            "  Epochs: 30\n",
            "  Batch size: 2000 residues\n",
            "  Training examples: 1620\n",
            "  Mixed precision: True\n",
            "  Checkpoint: /content/ProteinMPNN/vanilla_model_weights/v_48_002_wrapped_withopt.pt\n",
            "  Output: /content/finetune_output\n",
            "\n",
            "[override] dataloader_workers=0\n",
            "[override] Loading get_pdbs in-process (no ProcessPoolExecutor)...\n",
            "[override] got train=479 valid=60\n",
            "epoch: 1, step: 45, time: 4.4, train: 12.202, valid: 6.427, train_acc: 0.226, valid_acc: 0.414\n",
            "epoch: 2, step: 89, time: 4.6, train: 11.573, valid: 6.411, train_acc: 0.238, valid_acc: 0.416\n",
            "epoch: 3, step: 133, time: 4.6, train: 11.071, valid: 6.506, train_acc: 0.254, valid_acc: 0.413\n",
            "epoch: 4, step: 178, time: 4.4, train: 10.635, valid: 6.571, train_acc: 0.266, valid_acc: 0.413\n",
            "epoch: 5, step: 223, time: 4.6, train: 10.325, valid: 6.550, train_acc: 0.275, valid_acc: 0.412\n",
            "epoch: 6, step: 268, time: 4.6, train: 10.010, valid: 6.666, train_acc: 0.285, valid_acc: 0.409\n",
            "epoch: 7, step: 313, time: 4.6, train: 9.703, valid: 6.704, train_acc: 0.294, valid_acc: 0.406\n",
            "epoch: 8, step: 358, time: 4.5, train: 9.619, valid: 6.792, train_acc: 0.298, valid_acc: 0.401\n",
            "epoch: 9, step: 404, time: 4.7, train: 9.301, valid: 6.637, train_acc: 0.308, valid_acc: 0.410\n",
            "epoch: 10, step: 448, time: 4.7, train: 9.256, valid: 6.729, train_acc: 0.308, valid_acc: 0.406\n",
            "epoch: 11, step: 493, time: 4.4, train: 9.125, valid: 6.570, train_acc: 0.314, valid_acc: 0.412\n",
            "epoch: 12, step: 539, time: 4.7, train: 8.872, valid: 6.794, train_acc: 0.322, valid_acc: 0.400\n",
            "epoch: 13, step: 584, time: 4.5, train: 8.940, valid: 6.736, train_acc: 0.321, valid_acc: 0.403\n"
          ]
        }
      ],
      "source": [
        "#@title Cell 8: Run Fine-Tuning\n",
        "import sys, importlib\n",
        "if \"/content\" not in sys.path:\n",
        "    sys.path.insert(0, \"/content\")  # to find training_override.py\n",
        "# keep ProteinMPNN/training on sys.path too (already inserted earlier)\n",
        "import training_override\n",
        "importlib.reload(training_override)\n",
        "run_training = training_override.main\n",
        "\n",
        "class FinetuneArgs:\n",
        "    path_for_training_data = CONVERTED_DIR\n",
        "    path_for_outputs = OUTPUT_DIR\n",
        "    previous_checkpoint = wrapped_ckpt3\n",
        "    num_epochs = NUM_EPOCHS\n",
        "    save_model_every_n_epochs = SAVE_EVERY_N_EPOCHS\n",
        "    reload_data_every_n_epochs = RELOAD_DATA_EVERY_N\n",
        "    num_examples_per_epoch = n_train\n",
        "    batch_size = BATCH_SIZE\n",
        "    max_protein_length = MAX_PROTEIN_LENGTH\n",
        "    hidden_dim = HIDDEN_DIM\n",
        "    num_encoder_layers = NUM_ENCODER_LAYERS\n",
        "    num_decoder_layers = NUM_ENCODER_LAYERS\n",
        "    num_neighbors = NUM_NEIGHBORS\n",
        "    dropout = DROPOUT\n",
        "    backbone_noise = BACKBONE_NOISE\n",
        "    rescut = 3.5\n",
        "    debug = False\n",
        "    gradient_norm = -1.0\n",
        "    mixed_precision = torch.cuda.is_available()\n",
        "\n",
        "    # NEW: control multiprocessing without touching git repo\n",
        "    dataloader_workers = 0\n",
        "\n",
        "\n",
        "args = FinetuneArgs()\n",
        "print('=== Starting Fine-Tuning ===')\n",
        "print(f'  Epochs: {args.num_epochs}')\n",
        "print(f'  Batch size: {args.batch_size} residues')\n",
        "print(f'  Training examples: {args.num_examples_per_epoch}')\n",
        "print(f'  Mixed precision: {args.mixed_precision}')\n",
        "print(f'  Checkpoint: {args.previous_checkpoint}')\n",
        "print(f'  Output: {args.path_for_outputs}')\n",
        "print()\n",
        "\n",
        "run_training(args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nK72l0uHhCHR",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Cell 9: Training Curves\n",
        "\n",
        "log_path = os.path.join(OUTPUT_DIR, 'log.txt')\n",
        "if not os.path.exists(log_path):\n",
        "    print(f'Log file not found: {log_path}')\n",
        "else:\n",
        "    # Parse log.txt\n",
        "    epochs, train_ppl, valid_ppl, train_acc, valid_acc = [], [], [], [], []\n",
        "    with open(log_path) as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if line.startswith('Epoch') or not line:\n",
        "                continue\n",
        "            parts = {}\n",
        "            for item in line.split(','):\n",
        "                item = item.strip()\n",
        "                if ':' in item:\n",
        "                    k, v = item.split(':', 1)\n",
        "                    parts[k.strip()] = v.strip()\n",
        "            if 'epoch' in parts:\n",
        "                epochs.append(int(parts['epoch']))\n",
        "                train_ppl.append(float(parts.get('train', 'nan')))\n",
        "                valid_ppl.append(float(parts.get('valid', 'nan')))\n",
        "                train_acc.append(float(parts.get('train_acc', 'nan')))\n",
        "                valid_acc.append(float(parts.get('valid_acc', 'nan')))\n",
        "\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "    ax1.plot(epochs, train_ppl, 'b-', label='Train', linewidth=1.5)\n",
        "    ax1.plot(epochs, valid_ppl, 'r-', label='Validation', linewidth=1.5)\n",
        "    ax1.set_xlabel('Epoch')\n",
        "    ax1.set_ylabel('Perplexity')\n",
        "    ax1.set_title('Training Perplexity')\n",
        "    ax1.legend()\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "\n",
        "    ax2.plot(epochs, train_acc, 'b-', label='Train', linewidth=1.5)\n",
        "    ax2.plot(epochs, valid_acc, 'r-', label='Validation', linewidth=1.5)\n",
        "    ax2.set_xlabel('Epoch')\n",
        "    ax2.set_ylabel('Accuracy')\n",
        "    ax2.set_title('Sequence Recovery Accuracy')\n",
        "    ax2.legend()\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(OUTPUT_DIR, 'training_curves.png'), dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "    print(f'\\nFinal epoch {epochs[-1]}:')\n",
        "    print(f'  Train perplexity: {train_ppl[-1]:.3f}, accuracy: {train_acc[-1]:.3f}')\n",
        "    print(f'  Valid perplexity: {valid_ppl[-1]:.3f}, accuracy: {valid_acc[-1]:.3f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9KmKC44UhCHR",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Cell 10: design_from_seq_and_coords() API\n",
        "import os, sys, glob\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "# Ensure ProteinMPNN inference utils are on path\n",
        "if \"ProteinMPNN\" not in sys.path:\n",
        "    sys.path.insert(0, \"ProteinMPNN\")\n",
        "\n",
        "from protein_mpnn_utils import ProteinMPNN as InferenceModel\n",
        "from protein_mpnn_utils import StructureDatasetPDB, tied_featurize\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n",
        "\n",
        "# --- Load inference model from checkpoint ---\n",
        "def load_model(checkpoint_path, device):\n",
        "    ckpt = torch.load(checkpoint_path, map_location=device, weights_only=False)\n",
        "    state = ckpt[\"model_state_dict\"] if isinstance(ckpt, dict) and \"model_state_dict\" in ckpt else ckpt\n",
        "\n",
        "    model = InferenceModel(\n",
        "        num_letters=21,\n",
        "        node_features=HIDDEN_DIM,\n",
        "        edge_features=HIDDEN_DIM,\n",
        "        hidden_dim=HIDDEN_DIM,\n",
        "        num_encoder_layers=NUM_ENCODER_LAYERS,\n",
        "        num_decoder_layers=NUM_ENCODER_LAYERS,\n",
        "        augment_eps=0.0,\n",
        "        k_neighbors=NUM_NEIGHBORS,\n",
        "        dropout=0.0,\n",
        "    )\n",
        "    model.load_state_dict(state, strict=True)\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "def find_latest_ft_checkpoint(output_dir):\n",
        "    cands = glob.glob(os.path.join(output_dir, \"**\", \"model_weights\", \"*.pt\"), recursive=True)\n",
        "    assert cands, f\"No fine-tuned checkpoints found under {output_dir}\"\n",
        "    return sorted(cands, key=os.path.getmtime)[-1]\n",
        "\n",
        "model_vanilla = load_model(PRETRAINED_WEIGHTS, device)\n",
        "ft_weights = find_latest_ft_checkpoint(OUTPUT_DIR)\n",
        "print(\"Fine-tuned checkpoint:\", ft_weights)\n",
        "model_finetuned = load_model(ft_weights, device)\n",
        "\n",
        "# --- Get protein ids from converted dataset ---\n",
        "conv_list = pd.read_csv(os.path.join(CONVERTED_DIR, \"list.csv\"))\n",
        "all_ids = conv_list[\"pdbid_chain\"].tolist()\n",
        "print(\"Total converted proteins:\", len(all_ids))\n",
        "\n",
        "# --- Map id -> backbone chain .pt ---\n",
        "pdb_root = Path(CONVERTED_DIR) / \"pdb\"\n",
        "\n",
        "def find_chain_pt(pdbid_chain):\n",
        "    pdbid = pdbid_chain.split(\"_\")[0]\n",
        "    shard = pdbid[1:3]\n",
        "    chain_pt = pdb_root / shard / f\"{pdbid}_A.pt\"\n",
        "    return chain_pt if chain_pt.exists() else None\n",
        "\n",
        "# --- Design function ---\n",
        "def design_from_seq_and_coords(model, seq, xyz_bb, protein_name, num_samples=8, temperature=0.1, device=\"cpu\"):\n",
        "    \"\"\"\n",
        "    xyz_bb: numpy array [L, 4, 3] with N, CA, C, O coordinates.\n",
        "    Returns list of designed sequences.\n",
        "    \"\"\"\n",
        "    if len(seq) == 0 or xyz_bb.shape[0] == 0:\n",
        "        return []\n",
        "\n",
        "    # Build coords dict exactly like parse_PDB does:\n",
        "    # coords_chain_A must be a dict with keys N_chain_A, CA_chain_A, C_chain_A, O_chain_A\n",
        "    coords_dict = {\n",
        "        'N_chain_A':  xyz_bb[:, 0, :].tolist(),   # [L, 3]\n",
        "        'CA_chain_A': xyz_bb[:, 1, :].tolist(),\n",
        "        'C_chain_A':  xyz_bb[:, 2, :].tolist(),\n",
        "        'O_chain_A':  xyz_bb[:, 3, :].tolist(),\n",
        "    }\n",
        "\n",
        "    protein_dict = {\n",
        "        'name': protein_name,\n",
        "        'num_of_chains': 1,\n",
        "        'seq': seq,\n",
        "        'seq_chain_A': seq,\n",
        "        'coords_chain_A': coords_dict,\n",
        "    }\n",
        "\n",
        "    dataset = StructureDatasetPDB([protein_dict], max_length=MAX_PROTEIN_LENGTH)\n",
        "\n",
        "    alphabet = \"ACDEFGHIKLMNPQRSTVWYX\"\n",
        "    sequences = []\n",
        "\n",
        "    for protein in dataset:\n",
        "        batch = [protein]\n",
        "        pname = protein[\"name\"]\n",
        "        chain_dict = {pname: ([\"A\"], [])}\n",
        "\n",
        "        (\n",
        "            X, S, mask, lengths, chain_M, chain_encoding_all,\n",
        "            chain_list_list, visible_list_list, masked_list_list,\n",
        "            masked_chain_length_list_list, chain_M_pos, omit_AA_mask,\n",
        "            residue_idx, dihedral_mask, tied_pos_list_of_lists_list,\n",
        "            pssm_coef, pssm_bias, pssm_log_odds_all,\n",
        "            bias_by_res_all, tied_beta\n",
        "        ) = tied_featurize(\n",
        "            batch, device, chain_dict,\n",
        "            fixed_position_dict=None,\n",
        "            omit_AA_dict=None,\n",
        "            tied_positions_dict=None,\n",
        "            pssm_dict=None,\n",
        "            bias_by_res_dict=None,\n",
        "            ca_only=False,\n",
        "        )\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for _ in range(num_samples):\n",
        "                sample = model.sample(\n",
        "                    X, lengths * 0 + 1, mask, chain_M, residue_idx,\n",
        "                    chain_encoding_all, temperature=temperature\n",
        "                )\n",
        "                S_sample = sample[\"S\"][0].detach().cpu().numpy()\n",
        "                L = int(lengths[0])\n",
        "                s = \"\".join(alphabet[int(aa)] for aa in S_sample[:L])\n",
        "                sequences.append(s)\n",
        "\n",
        "    return sequences\n",
        "\n",
        "# --- Run designs on a subset ---\n",
        "n_test = min(16, len(all_ids))\n",
        "num_samples = 8\n",
        "temperature = 0.1\n",
        "\n",
        "all_designs = []\n",
        "\n",
        "for i, pdbid_chain in enumerate(all_ids[:n_test]):\n",
        "    chain_pt = find_chain_pt(pdbid_chain)\n",
        "    if chain_pt is None:\n",
        "        continue\n",
        "\n",
        "    data = torch.load(chain_pt, weights_only=False)\n",
        "    seq = data[\"seq\"]\n",
        "    xyz = data[\"xyz\"].numpy()  # [L, 4, 3]\n",
        "\n",
        "    print(f\"[{i+1}/{n_test}] Designing for {pdbid_chain} (L={len(seq)})\")\n",
        "\n",
        "    vanilla_seqs = design_from_seq_and_coords(model_vanilla, seq, xyz, pdbid_chain, num_samples, temperature, device)\n",
        "    ft_seqs = design_from_seq_and_coords(model_finetuned, seq, xyz, pdbid_chain, num_samples, temperature, device)\n",
        "\n",
        "    prot_id = pdbid_chain.split(\"_\")[0]\n",
        "    for j, s in enumerate(vanilla_seqs):\n",
        "        all_designs.append({\"protein\": prot_id, \"model\": \"vanilla\", \"sample\": j, \"sequence\": s})\n",
        "    for j, s in enumerate(ft_seqs):\n",
        "        all_designs.append({\"protein\": prot_id, \"model\": \"finetuned\", \"sample\": j, \"sequence\": s})\n",
        "\n",
        "designs_df = pd.DataFrame(all_designs)\n",
        "out_csv = os.path.join(OUTPUT_DIR, \"designs_vanilla_vs_finetuned.csv\")\n",
        "designs_df.to_csv(out_csv, index=False)\n",
        "\n",
        "print(f\"\\nDesign generation complete\")\n",
        "print(f\"Total designs: {len(designs_df)}\")\n",
        "print(f\"Saved to: {out_csv}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5d-QuU-vhCHR",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Cell 11: Evaluate pH Feature Shifts (COMPATIBLE VERSION)\n",
        "#\n",
        "# Reads OUTPUT_DIR/designs_vanilla_vs_finetuned.csv from Cell 9\n",
        "# Writes:\n",
        "#   OUTPUT_DIR/ph_feature_comparison.csv\n",
        "#   OUTPUT_DIR/ph_feature_effect_sizes.png\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Biopython is already installed in your notebook earlier; SciPy might not be.\n",
        "try:\n",
        "    from scipy import stats\n",
        "except ImportError:\n",
        "    !pip -q install scipy\n",
        "    from scipy import stats\n",
        "\n",
        "from Bio.SeqUtils.ProtParam import ProteinAnalysis\n",
        "\n",
        "def calculate_ph_features(sequence: str):\n",
        "    \"\"\"\n",
        "    Returns dict of per-sequence features or None on failure.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        pa = ProteinAnalysis(sequence)\n",
        "\n",
        "        L = len(sequence)\n",
        "        if L == 0:\n",
        "            return None\n",
        "\n",
        "        mw = pa.molecular_weight() / L\n",
        "        ip = pa.isoelectric_point()\n",
        "        gravy = pa.gravy()\n",
        "        aromaticity = pa.aromaticity()\n",
        "        instability = pa.instability_index()\n",
        "\n",
        "        charge_7 = pa.charge_at_pH(7.0)\n",
        "        charge_per_res = charge_7 / L\n",
        "\n",
        "        acidic = sum(1 for aa in sequence if aa in \"DE\") / L\n",
        "        basic = sum(1 for aa in sequence if aa in \"RKH\") / L\n",
        "        ionizable = sum(1 for aa in sequence if aa in \"DEHRKCY\") / L\n",
        "\n",
        "        return {\n",
        "            \"mw_per_residue\": mw,\n",
        "            \"isoelectric_point\": ip,\n",
        "            \"gravy\": gravy,\n",
        "            \"aromaticity\": aromaticity,\n",
        "            \"instability_index\": instability,\n",
        "            \"charge_per_residue\": charge_per_res,\n",
        "            \"acidic_residue_fraction\": acidic,\n",
        "            \"basic_residue_fraction\": basic,\n",
        "            \"ionizable_residue_fraction\": ionizable,\n",
        "        }\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "designs_csv = os.path.join(OUTPUT_DIR, \"designs_vanilla_vs_finetuned.csv\")\n",
        "assert os.path.exists(designs_csv), f\"Designs file not found: {designs_csv} (run Cell 9 first)\"\n",
        "\n",
        "designs_df = pd.read_csv(designs_csv)\n",
        "assert {\"protein\",\"model\",\"sample\",\"sequence\"}.issubset(designs_df.columns), \"Designs CSV missing required columns\"\n",
        "\n",
        "features_list = []\n",
        "for _, row in designs_df.iterrows():\n",
        "    feats = calculate_ph_features(str(row[\"sequence\"]))\n",
        "    if feats is None:\n",
        "        continue\n",
        "    feats[\"protein\"] = row[\"protein\"]\n",
        "    feats[\"model\"] = row[\"model\"]\n",
        "    feats[\"sample\"] = row[\"sample\"]\n",
        "    features_list.append(feats)\n",
        "\n",
        "feat_df = pd.DataFrame(features_list)\n",
        "print(\"Computed features for:\", len(feat_df), \"designs\")\n",
        "\n",
        "ph_features = [\n",
        "    \"isoelectric_point\",\n",
        "    \"charge_per_residue\",\n",
        "    \"acidic_residue_fraction\",\n",
        "    \"basic_residue_fraction\",\n",
        "    \"ionizable_residue_fraction\",\n",
        "    \"gravy\",\n",
        "    \"aromaticity\",\n",
        "    \"instability_index\",\n",
        "    \"mw_per_residue\",\n",
        "]\n",
        "\n",
        "print(\"\\n=== pH Feature Comparison: Vanilla vs Fine-Tuned ===\")\n",
        "print(f'{\"Feature\":<30} {\"Vanilla\":>10} {\"FT\":>10} {\"Diff\":>10} {\"Cohen d\":>10} {\"p-value\":>10}')\n",
        "print(\"-\" * 84)\n",
        "\n",
        "results = []\n",
        "for feat in ph_features:\n",
        "    van = feat_df.loc[feat_df[\"model\"] == \"vanilla\", feat].dropna()\n",
        "    ft = feat_df.loc[feat_df[\"model\"] == \"finetuned\", feat].dropna()\n",
        "\n",
        "    if len(van) < 2 or len(ft) < 2:\n",
        "        continue\n",
        "\n",
        "    t_stat, p_val = stats.ttest_ind(van, ft, equal_var=False)  # Welch t-test\n",
        "    pooled_sd = np.sqrt((van.var(ddof=1) + ft.var(ddof=1)) / 2)\n",
        "    cohens_d = (ft.mean() - van.mean()) / pooled_sd if pooled_sd > 0 else np.nan\n",
        "\n",
        "    sig = \"***\" if p_val < 0.001 else \"**\" if p_val < 0.01 else \"*\" if p_val < 0.05 else \"\"\n",
        "    print(f\"{feat:<30} {van.mean():>10.4f} {ft.mean():>10.4f} {(ft.mean()-van.mean()):>+10.4f} {cohens_d:>10.3f} {p_val:>9.2e} {sig}\")\n",
        "\n",
        "    results.append({\n",
        "        \"feature\": feat,\n",
        "        \"vanilla_mean\": float(van.mean()),\n",
        "        \"finetuned_mean\": float(ft.mean()),\n",
        "        \"difference\": float(ft.mean() - van.mean()),\n",
        "        \"cohens_d\": float(cohens_d) if np.isfinite(cohens_d) else np.nan,\n",
        "        \"p_value\": float(p_val),\n",
        "        \"n_vanilla\": int(len(van)),\n",
        "        \"n_finetuned\": int(len(ft)),\n",
        "    })\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "out_stats = os.path.join(OUTPUT_DIR, \"ph_feature_comparison.csv\")\n",
        "results_df.to_csv(out_stats, index=False)\n",
        "print(\"\\nWrote:\", out_stats)\n",
        "\n",
        "# Effect size plot (no manual colors; keep default matplotlib cycle)\n",
        "if len(results_df) > 0:\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "    ax.barh(results_df[\"feature\"], results_df[\"cohens_d\"])\n",
        "    ax.axvline(x=0, linewidth=0.8)\n",
        "    ax.set_xlabel(\"Cohen's d (Fine-tuned - Vanilla)\")\n",
        "    ax.set_title(\"Effect of Fine-Tuning on Biophysical Properties\")\n",
        "    plt.tight_layout()\n",
        "    out_png = os.path.join(OUTPUT_DIR, \"ph_feature_effect_sizes.png\")\n",
        "    plt.savefig(out_png, dpi=300, bbox_inches=\"tight\")\n",
        "    plt.show()\n",
        "    print(\"Wrote:\", out_png)\n",
        "else:\n",
        "    print(\"No results to plot (insufficient samples?).\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3_0VEkBAhCHS",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Cell 12: Save & Export\n",
        "\n",
        "print('=== Output Summary ===')\n",
        "for f in sorted(Path(OUTPUT_DIR).rglob('*')):\n",
        "    if f.is_file():\n",
        "        size_kb = f.stat().st_size / 1024\n",
        "        print(f'  {f.relative_to(OUTPUT_DIR)} ({size_kb:.1f} KB)')\n",
        "\n",
        "# Zip for download\n",
        "zip_path = f'{OUTPUT_DIR}.zip'\n",
        "shutil.make_archive(OUTPUT_DIR, 'zip', OUTPUT_DIR)\n",
        "print(f'\\nZipped to: {zip_path}')\n",
        "print('\\n=== FINE-TUNING COMPLETE ===')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2R1SaKg5hCHS"
      },
      "source": [
        "## Troubleshooting\n",
        "\n",
        "**\"CUDA out of memory\"**: Reduce `BATCH_SIZE` (e.g., 500) or `MAX_PROTEIN_LENGTH` (e.g., 500).\n",
        "\n",
        "**`loader_pdb` returns dummy arrays / `get_pdbs` returns 0 proteins**: The `.pt` files weren't placed correctly. Check:\n",
        "- Files must be **flat** in the shard directory: `pdb/{pdbid[1:3]}/{pdbid}.pt` and `pdb/{pdbid[1:3]}/{pdbid}_A.pt`\n",
        "- **NOT** inside a per-protein subfolder like `pdb/0a/a0a087wnh4/a0a087wnh4.pt`\n",
        "- Metadata `.pt` must be a dict with `asmb_ids`, `asmb_chains`, `chains`, `tm` keys\n",
        "- Chain `.pt` must be a dict with `seq` (str) and `xyz` (tensor [L, 14, 3])\n",
        "- `list.csv` must have columns: pdbid_chain, date, resolution, **chain_id**, cluster_id (in that order)\n",
        "\n",
        "**Perplexity is NaN/Inf**: Likely a data issue — check that sequences contain only standard amino acids (ACDEFGHIKLMNPQRSTVWY).\n",
        "\n",
        "**Low accuracy (~5%)**: Expected at start of training from scratch. When fine-tuning from pretrained weights, initial accuracy should be ~30-40%. If it drops to ~5%, the checkpoint may not have loaded correctly.\n",
        "\n",
        "**Mixed precision errors on CPU/MPS**: Set `mixed_precision = False` in configuration."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}