{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA Analysis Suite\n",
    "\n",
    "Consolidated analysis notebook for *Decoding the physicochemical basis of taxonomy preferences in protein design models*.\n",
    "\n",
    "## Analyses Covered\n",
    "\n",
    "| Section | Paper Reference | Description |\n",
    "|---------|----------------|-------------|\n",
    "| **WT PCA** | Figure 3A-B | PCA on 4 feature sets (mixed, sequence, structure, pH) |\n",
    "| **GAM Landscapes** | Figure 3C | Model preference surfaces in PC space |\n",
    "| **Cosine Similarity** | Methods §Cosine Similarity | Likelihood vectors vs feature loading vectors |\n",
    "| **Fine-tuning Validation** | Results §Fine-tuning | Vanilla vs AlkalineMPNN pH alignment |\n",
    "| **Design Shift PCA** | Figure 4 | Designed sequences projected into WT PC space |\n",
    "| **Quantitative Shifts** | Supplementary Table S12 | Centroid shifts, back-projection, model cosine similarity |\n",
    "\n",
    "## Requirements\n",
    "R packages: tidyverse, ggrepel, mgcv, patchwork, rlang, MASS, viridis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Cell 2: Setup & Packages\npackages <- c(\"tidyverse\", \"ggrepel\", \"mgcv\", \"patchwork\", \"rlang\", \"MASS\", \"viridis\")\nnew_pkgs <- packages[!(packages %in% installed.packages()[, \"Package\"])]\nif (length(new_pkgs)) install.packages(new_pkgs, quiet = TRUE)\n\nsuppressPackageStartupMessages({\n  library(MASS)       # Load before tidyverse so dplyr::select is not masked\n  library(tidyverse)\n  library(ggrepel)\n  library(mgcv)\n  library(patchwork)\n  library(rlang)\n  library(viridis)\n})\n\ncat(\"Setup complete\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Cell 3: Configuration — EDIT PATHS HERE\n\n# Wild-type protein dataset (7843 proteins with model scores and features)\nWT_CSV <- \"/Users/lauradillon/PycharmProjects/inverse_fold/Cleaned_research_flow/0_Main_data/Final_Paper_Folder/Decoding_Bias_Dataset.csv\"\n\n# Design data: individual per-model CSVs with shift columns\nDESIGN_DIR <- \"design_data\"  # directory containing per-model design CSVs\nDESIGN_FILES <- list(\n  \"ProteinMPNN\" = file.path(DESIGN_DIR, \"ProteinMPNN_designs_with_WT_shifts.csv\"),\n  \"ESM-IF\"      = file.path(DESIGN_DIR, \"ESMIF_designs_with_WT_shifts.csv\"),\n  \"MIF\"         = file.path(DESIGN_DIR, \"MIF_designs_with_WT_shifts.csv\"),\n  \"MIF-ST\"      = file.path(DESIGN_DIR, \"MIFST_designs_with_WT_shifts.csv\"),\n  \"AlkalineMPNN\"= file.path(DESIGN_DIR, \"Alkaline_designs_with_WT_shifts.csv\")\n)\n\nOUTPUT_DIR <- \"pca_analysis_results\"\ndir.create(OUTPUT_DIR, showWarnings = FALSE, recursive = TRUE)\n\n# Color palettes\nDOMAIN_COLORS <- c(\n  \"Bacteria\"  = \"#1f77b4\",\n  \"Eukaryota\" = \"#2ca02c\",\n  \"Archaea\"   = \"#eb0920\"\n)\n\nMODEL_COLORS <- c(\n  \"ProteinMPNN\"  = \"#1F77B4\",\n  \"AlkalineMPNN\" = \"#E15759\",\n  \"ESM-IF\"       = \"#FF7F0E\",\n  \"MIF\"          = \"#9467BD\",\n  \"MIF-ST\"       = \"#2CA02C\"\n)\n\n# Model score columns in the WT dataset\nSCORE_COLUMNS <- c(\n  \"proteinmpnn_score\", \"esmif_score\", \"mif_score\",\n  \"mifst_score\", \"ESM2_15B_pppl_score\", \"carp_640M_score\",\n  \"AlkalineMPNN_score\"\n)\n\ncat(\"Configuration set.\\n\")\ncat(\"Output directory:\", OUTPUT_DIR, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": "# Cell 4: Helper Functions\n\n# Invert structural features so higher = more compact/centralized\ncreate_inverse_structural_features <- function(df) {\n  if (\"compactness\" %in% names(df)) {\n    df$radius_of_gyration <- df$compactness\n    df$structural_compactness <- 1 / df$compactness\n  }\n  if (\"avg_cb_distance\" %in% names(df)) {\n    df$avg_cb_distance_from_centroid <- df$avg_cb_distance\n    df$centralization <- 1 / df$avg_cb_distance\n  }\n  df\n}\n\n# Unit vector (for cosine similarity)\n.unit_vec <- function(v) {\n  v <- as.numeric(v)\n  n <- sqrt(sum(v^2))\n  if (!is.finite(n) || n < 1e-12) stop(\"Zero-length vector\")\n  v / n\n}\n\n# Cosine similarity between two vectors\n.cosine <- function(a, b) sum(.unit_vec(a) * .unit_vec(b))\n\n# Vectorized cosine similarity (handles NAs)\n.cosine2 <- function(dx1, dy1, dx2, dy2, eps = 1e-12) {\n  n1 <- sqrt(dx1^2 + dy1^2)\n  n2 <- sqrt(dx2^2 + dy2^2)\n  ok <- (n1 > eps) & (n2 > eps)\n  out <- rep(NA_real_, length(dx1))\n  out[ok] <- (dx1[ok]*dx2[ok] + dy1[ok]*dy2[ok]) / (n1[ok]*n2[ok])\n  pmin(1, pmax(-1, out))\n}\n\n# Safe numeric coercion\n.as_num <- function(x) suppressWarnings(as.numeric(x))\n\n# PC axis labels with variance %\npc_axis_labels <- function(explained_variance, pc_x = \"PC1\", pc_y = \"PC2\") {\n  ix <- as.integer(gsub(\"PC\", \"\", pc_x))\n  iy <- as.integer(gsub(\"PC\", \"\", pc_y))\n  list(\n    x = sprintf(\"%s (%.1f%%)\", pc_x, 100 * explained_variance[ix]),\n    y = sprintf(\"%s (%.1f%%)\", pc_y, 100 * explained_variance[iy])\n  )\n}\n\n# Symmetric axis limits for PC plots (centered on 0)\npc_symmetric_limits <- function(data, pc_x = \"PC1\", pc_y = \"PC2\", expand = 0.05) {\n  xr <- range(data[[pc_x]], na.rm = TRUE)\n  yr <- range(data[[pc_y]], na.rm = TRUE)\n  mx <- max(abs(xr)) * (1 + expand)\n  my <- max(abs(yr)) * (1 + expand)\n  list(x = c(-mx, mx), y = c(-my, my))\n}\n\n# Bounded aspect ratio from variance explained\ncalculate_bounded_aspect <- function(explained_variance, pc_x = 1, pc_y = 2,\n                                      lo = 0.4, hi = 2.5) {\n  ratio <- explained_variance[pc_y] / explained_variance[pc_x]\n  max(lo, min(hi, ratio))\n}\n\ncat(\"Helper functions loaded.\\n\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Cell 5: Feature Set Definitions\n\nsequence_features <- c(\n  \"mw_per_residue\", \"isoelectric_point\", \"instability_index\",\n  \"gravy\", \"sequence_length\", \"aromaticity\"\n)\n\nstructure_features <- c(\n  \"helix_sheet_contrast\", \"ordered_percent\", \"rco\",\n  \"structural_compactness\", \"centralization\"\n)\n\nmixed_features <- c(sequence_features, structure_features)\n\npH_features <- c(\n  \"buffer_capacity\", \"charge_per_residue\",\n  \"acidic_residue_fraction\", \"basic_residue_fraction\",\n  \"ionizable_residue_fraction\"\n)\n\ncat(\"Feature sets defined:\\n\")\ncat(\"  Sequence:\", length(sequence_features), \"features\\n\")\ncat(\"  Structure:\", length(structure_features), \"features\\n\")\ncat(\"  Mixed:\", length(mixed_features), \"features\\n\")\ncat(\"  pH:\", length(pH_features), \"features\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Cell 6: Data Loading & Preprocessing\n\ncat(\"Loading data from:\", WT_CSV, \"\\n\")\ndf <- read.csv(WT_CSV, stringsAsFactors = FALSE)\ncat(\"Loaded\", nrow(df), \"proteins with\", ncol(df), \"columns\\n\")\n\n# Create inverse structural features\ndf <- create_inverse_structural_features(df)\n\n# Create derived structure columns if not present\nif (all(c(\"helix_percent\", \"sheet_percent\") %in% names(df))) {\n  if (!\"helix_sheet_contrast\" %in% names(df)) {\n    df$helix_sheet_contrast <- df$helix_percent - df$sheet_percent\n  }\n  if (!\"ordered_percent\" %in% names(df)) {\n    df$ordered_percent <- df$helix_percent + df$sheet_percent\n  }\n}\n\n# Verify score columns\navailable_scores <- SCORE_COLUMNS[SCORE_COLUMNS %in% names(df)]\ncat(\"Available model score columns:\", length(available_scores), \"\\n\")\ncat(\" \", paste(available_scores, collapse = \", \"), \"\\n\")\n\n# Verify feature columns\nfor (fs_name in c(\"mixed_features\", \"pH_features\")) {\n  fs <- get(fs_name)\n  present <- fs[fs %in% names(df)]\n  missing <- fs[!fs %in% names(df)]\n  cat(sprintf(\"%s: %d/%d present\", fs_name, length(present), length(fs)))\n  if (length(missing)) cat(\" (missing:\", paste(missing, collapse = \", \"), \")\")\n  cat(\"\\n\")\n}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Cell 7: Core PCA Function & Feature Contribution\n\nperform_focused_pca <- function(df, features, anchor_var = c(PC1 = \"sequence_length\")) {\n  cat(\"\\n=== PCA ANALYSIS ===\", \"\\n\")\n  cat(\"Using\", length(features), \"features\\n\")\n\n  # Required metadata columns\n  score_cols <- SCORE_COLUMNS[SCORE_COLUMNS %in% colnames(df)]\n  meta_cols <- c(\"Entry\", \"domain\", \"species\", \"protein_name\", \"avg_plddt\")\n  meta_cols <- meta_cols[meta_cols %in% colnames(df)]\n\n  # Prepare data\n  keep_cols <- unique(c(features, score_cols, meta_cols))\n  pca_data <- df[, intersect(keep_cols, names(df)), drop = FALSE]\n\n  # Remove rows with NA/Inf in feature columns\n  for (col in features) {\n    if (col %in% names(pca_data)) {\n      pca_data[[col]][is.infinite(pca_data[[col]])] <- NA\n    }\n  }\n  initial_rows <- nrow(pca_data)\n  pca_data <- pca_data[complete.cases(pca_data[, features, drop = FALSE]), ]\n  cat(\"Clean data:\", nrow(pca_data), \"samples (removed\", initial_rows - nrow(pca_data), \"rows)\\n\")\n\n  # Standardize and run PCA\n  feature_matrix <- as.matrix(pca_data[, features, drop = FALSE])\n  center_vals <- colMeans(feature_matrix)\n  scale_vals <- apply(feature_matrix, 2, sd)\n  scaled_matrix <- scale(feature_matrix, center = center_vals, scale = scale_vals)\n\n  pca_result <- prcomp(scaled_matrix, center = FALSE, scale. = FALSE)\n\n  # Explained variance\n  var_explained <- pca_result$sdev^2 / sum(pca_result$sdev^2)\n  cat(sprintf(\"Variance explained (PC1+PC2): %.3f\\n\", sum(var_explained[1:2])))\n\n  # Sign-flip anchoring: ensure anchor variable loads positively on its PC\n  for (pc_name in names(anchor_var)) {\n    pc_idx <- as.integer(gsub(\"PC\", \"\", pc_name))\n    var_name <- anchor_var[[pc_name]]\n    if (var_name %in% features && var_name %in% rownames(pca_result$rotation)) {\n      if (pca_result$rotation[var_name, pc_idx] < 0) {\n        pca_result$rotation[, pc_idx] <- -pca_result$rotation[, pc_idx]\n        pca_result$x[, pc_idx] <- -pca_result$x[, pc_idx]\n      }\n    }\n  }\n\n  # Assemble PC scores with metadata\n  pc_scores <- as.data.frame(pca_result$x[, 1:min(ncol(pca_result$x), 10)])\n  result_data <- bind_cols(pca_data, pc_scores)\n\n  # Loadings table\n  loadings_df <- data.frame(\n    Feature = features,\n    PC1 = pca_result$rotation[features, 1],\n    PC2 = pca_result$rotation[features, 2],\n    stringsAsFactors = FALSE\n  )\n  if (ncol(pca_result$rotation) >= 3) {\n    loadings_df$PC3 <- pca_result$rotation[features, 3]\n  }\n\n  list(\n    data = result_data,\n    loadings = loadings_df,\n    explained_variance = var_explained,\n    features = features,\n    center = center_vals,\n    scale = scale_vals,\n    pca_object = pca_result\n  )\n}\n\n# Feature contribution to PC variance (Methods equation)\ncalculate_feature_contributions <- function(pca_results, n_pcs = 2) {\n  loadings <- pca_results$pca_object$rotation\n  var_exp <- pca_results$explained_variance\n  features <- pca_results$features\n\n  contributions <- sapply(features, function(f) {\n    sum(loadings[f, 1:n_pcs]^2 * var_exp[1:n_pcs])\n  })\n\n  tibble(\n    feature = features,\n    contribution = contributions,\n    contribution_pct = 100 * contributions / sum(contributions)\n  ) %>% arrange(desc(contribution))\n}\n\ncat(\"PCA functions loaded.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": "# Cell 8: Plotting Functions\n\n# Figure 3A: Domain-colored scatter in PC space\ncreate_protein_scatter <- function(pca_results, pc_x = \"PC1\", pc_y = \"PC2\",\n                                   show_density_contours = FALSE) {\n  labs_xy <- pc_axis_labels(pca_results$explained_variance, pc_x, pc_y)\n  ix <- as.integer(gsub(\"PC\", \"\", pc_x))\n  iy <- as.integer(gsub(\"PC\", \"\", pc_y))\n  ratio_yx <- pca_results$explained_variance[iy] / pca_results$explained_variance[ix]\n\n  p <- ggplot(pca_results$data, aes(x = .data[[pc_x]], y = .data[[pc_y]],\n                                     color = domain)) +\n    geom_point(alpha = 0.5, size = 1.2) +\n    scale_color_manual(values = DOMAIN_COLORS, name = \"Domain\") +\n    labs(x = labs_xy$x, y = labs_xy$y,\n         title = \"Proteins in biophysical PC space\") +\n    theme_minimal(base_size = 12) +\n    theme(legend.position = \"bottom\",\n          panel.border = element_rect(color = \"grey60\", fill = NA),\n          axis.title = element_text(face = \"bold\")) +\n    coord_fixed(ratio = ratio_yx) +\n    geom_hline(yintercept = 0, linetype = \"dashed\", color = \"grey70\") +\n    geom_vline(xintercept = 0, linetype = \"dashed\", color = \"grey70\")\n\n  if (show_density_contours) {\n    p <- p + geom_density_2d(color = \"grey40\", alpha = 0.3)\n  }\n  p\n}\n\n# Figure 3B: Feature loading biplot\ncreate_feature_biplot <- function(pca_results, pc_x = \"PC1\", pc_y = \"PC2\",\n                                   top_n_features = 8) {\n  labs_xy <- pc_axis_labels(pca_results$explained_variance, pc_x, pc_y)\n  loadings <- pca_results$loadings\n\n  # Scale arrows for visibility\n  arrow_scale <- max(abs(c(loadings[[pc_x]], loadings[[pc_y]]))) * 1.1\n  loadings$x_scaled <- loadings[[pc_x]] / arrow_scale\n  loadings$y_scaled <- loadings[[pc_y]] / arrow_scale\n  loadings$magnitude <- sqrt(loadings$x_scaled^2 + loadings$y_scaled^2)\n\n  top_loadings <- loadings %>% arrange(desc(magnitude)) %>% slice_head(n = top_n_features)\n\n  # Pretty feature names\n  pretty_name <- function(x) {\n    x <- gsub(\"_\", \" \", x)\n    x <- gsub(\"mw per residue\", \"MW/residue\", x)\n    x <- gsub(\"structural compactness\", \"compactness (1/Rg)\", x)\n    x <- gsub(\"centralization\", \"centralization (1/dCB)\", x)\n    tools::toTitleCase(x)\n  }\n  top_loadings$label <- sapply(top_loadings$Feature, pretty_name)\n\n  ggplot() +\n    geom_segment(data = top_loadings,\n                 aes(x = 0, y = 0, xend = x_scaled, yend = y_scaled),\n                 arrow = arrow(length = unit(0.15, \"inches\"), type = \"closed\"),\n                 color = \"steelblue\", linewidth = 0.8, alpha = 0.8) +\n    geom_text_repel(data = top_loadings,\n                    aes(x = x_scaled, y = y_scaled, label = label),\n                    size = 3.5, fontface = \"bold\", color = \"grey20\",\n                    box.padding = 0.4, max.overlaps = 20) +\n    labs(x = labs_xy$x, y = labs_xy$y, title = \"Feature loadings\") +\n    theme_minimal(base_size = 12) +\n    theme(panel.border = element_rect(color = \"grey60\", fill = NA),\n          axis.title = element_text(face = \"bold\")) +\n    coord_fixed() +\n    geom_hline(yintercept = 0, linetype = \"dashed\", color = \"grey70\") +\n    geom_vline(xintercept = 0, linetype = \"dashed\", color = \"grey70\")\n}\n\n# Scree plot\ncreate_variance_plot <- function(pca_results, n_components = 6) {\n  n <- min(n_components, length(pca_results$explained_variance))\n  var_df <- tibble(\n    PC = factor(paste0(\"PC\", 1:n), levels = paste0(\"PC\", 1:n)),\n    Variance = 100 * pca_results$explained_variance[1:n],\n    Cumulative = cumsum(100 * pca_results$explained_variance[1:n])\n  )\n\n  ggplot(var_df, aes(x = PC, y = Variance)) +\n    geom_col(fill = \"steelblue\", alpha = 0.7) +\n    geom_text(aes(label = sprintf(\"%.1f%%\", Variance)), vjust = -0.3, size = 3.5) +\n    geom_line(aes(y = Cumulative, group = 1), color = \"red\", linewidth = 0.8) +\n    geom_point(aes(y = Cumulative), color = \"red\", size = 2) +\n    labs(x = \"Principal Component\", y = \"Variance Explained (%)\",\n         title = \"Variance explained by principal components\") +\n    theme_minimal(base_size = 12)\n}\n\n# Figure 3C: GAM preference surface (matches original CLEAN_PCA-2 styling)\ncreate_gam_contour_plot <- function(pca_results, score_col, score_label = NULL,\n                                     pc_x = \"PC1\", pc_y = \"PC2\",\n                                     grid_size = 120, n_contours = 10, k_basis = NULL) {\n  if (is.null(score_label)) score_label <- score_col\n  labs_xy <- pc_axis_labels(pca_results$explained_variance, pc_x, pc_y)\n\n  data_clean <- pca_results$data %>%\n    dplyr::select(all_of(c(pc_x, pc_y, score_col))) %>%\n    drop_na()\n\n  N <- nrow(data_clean)\n  if (is.null(k_basis)) k_basis <- max(12, min(50, floor(N / 300)))\n\n  form <- as.formula(paste0(\"`\", score_col, \"` ~ s(\", pc_x, \", \", pc_y, \", k = \", k_basis, \")\"))\n  gam_fit <- gam(form, data = data_clean, method = \"REML\")\n\n  # Symmetric axis limits centered on 0\n  lims <- pc_symmetric_limits(data_clean, pc_x, pc_y, expand = 0.05)\n\n  grid <- expand.grid(\n    V1 = seq(lims$x[1], lims$x[2], length.out = grid_size),\n    V2 = seq(lims$y[1], lims$y[2], length.out = grid_size)\n  )\n  names(grid) <- c(pc_x, pc_y)\n  grid$pred <- predict(gam_fit, newdata = grid)\n\n  # Bounded aspect ratio from variance explained\n  aspect_ratio <- calculate_bounded_aspect(pca_results$explained_variance)\n\n  ggplot(grid, aes(x = .data[[pc_x]], y = .data[[pc_y]], z = pred)) +\n    geom_raster(aes(fill = pred), interpolate = TRUE) +\n    geom_contour(color = \"white\", alpha = 0.5, linewidth = 0.3, bins = n_contours) +\n    scale_fill_viridis_c(name = score_label, option = \"viridis\") +\n    labs(title = NULL, x = labs_xy$x, y = labs_xy$y) +\n    coord_cartesian(xlim = lims$x, ylim = lims$y, expand = FALSE) +\n    theme_minimal(base_size = 20) +\n    theme(\n      panel.grid = element_blank(),\n      axis.title = element_text(face = \"bold\", size = 20),\n      axis.text = element_text(size = 20),\n      aspect.ratio = 1 / aspect_ratio\n    )\n}\n\ncat(\"Plotting functions loaded.\\n\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Cell 9: Run WT PCA (all 4 feature sets)\n\nfeature_sets <- list(\n  \"Mixed Features\"     = mixed_features,\n  \"Sequence Features\"  = sequence_features,\n  \"Structure Features\" = structure_features,\n  \"pH Features\"        = pH_features\n)\n\nall_results <- list()\n\nfor (set_name in names(feature_sets)) {\n  cat(\"\\n=== ANALYZING:\", set_name, \"===\\n\")\n\n  features <- feature_sets[[set_name]]\n  available <- intersect(features, colnames(df))\n\n  if (length(available) < 3) {\n    cat(\"Skipping\", set_name, \": insufficient features\\n\")\n    next\n  }\n\n  pca_results <- perform_focused_pca(df, available)\n\n  # Feature contributions\n  contributions <- calculate_feature_contributions(pca_results)\n  cat(\"\\nFeature contributions to total PC variance:\\n\")\n  print(contributions, n = nrow(contributions))\n\n  # Create plots\n  p_scatter <- create_protein_scatter(pca_results)\n  p_biplot <- create_feature_biplot(pca_results, top_n_features = min(8, length(available)))\n  p_variance <- create_variance_plot(pca_results)\n\n  # Store\n  all_results[[set_name]] <- list(\n    pca_results = pca_results,\n    contributions = contributions,\n    plots = list(scatter = p_scatter, biplot = p_biplot, variance = p_variance)\n  )\n\n  # Display\n  print(p_scatter + p_biplot + plot_layout(widths = c(1.2, 1)))\n  print(p_variance)\n\n  # Save\n  prefix <- gsub(\" \", \"_\", set_name)\n  ggsave(file.path(OUTPUT_DIR, paste0(prefix, \"_scatter.png\")), p_scatter,\n         width = 8, height = 7, dpi = 300, bg = \"white\")\n  ggsave(file.path(OUTPUT_DIR, paste0(prefix, \"_biplot.png\")), p_biplot,\n         width = 7, height = 6, dpi = 300, bg = \"white\")\n\n  # Save loadings table\n  write_csv(pca_results$loadings,\n            file.path(OUTPUT_DIR, paste0(prefix, \"_loadings.csv\")))\n  write_csv(contributions,\n            file.path(OUTPUT_DIR, paste0(prefix, \"_contributions.csv\")))\n}\n\ncat(\"\\n=== WT PCA COMPLETE ===\")\ncat(\"\\nAnalyses completed:\", paste(names(all_results), collapse = \", \"), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Cell 10: GAM Preference Landscapes (Figure 3C)\n\n# Score display names\nscore_labels <- c(\n  proteinmpnn_score    = \"ProteinMPNN\",\n  esmif_score          = \"ESM-IF\",\n  mif_score            = \"MIF\",\n  mifst_score          = \"MIF-ST\",\n  ESM2_15B_pppl_score  = \"ESM2-15B\",\n  carp_640M_score      = \"CARP-640M\",\n  AlkalineMPNN_score   = \"AlkalineMPNN\"\n)\n\n# Generate GAM surfaces for each feature set\nfor (set_name in names(all_results)) {\n  cat(\"\\n=== GAM LANDSCAPES FOR:\", set_name, \"===\\n\")\n  pca_res <- all_results[[set_name]]$pca_results\n  scores_present <- intersect(available_scores, names(pca_res$data))\n\n  gam_plots <- list()\n  for (sc in scores_present) {\n    label <- ifelse(sc %in% names(score_labels), score_labels[sc], sc)\n    cat(\"  Fitting GAM for:\", label, \"\\n\")\n    p <- create_gam_contour_plot(pca_res, sc, label)\n    gam_plots[[sc]] <- p\n  }\n\n  # Display as grid\n  if (length(gam_plots) > 0) {\n    combined <- wrap_plots(gam_plots, ncol = min(3, length(gam_plots))) +\n      plot_annotation(title = paste(\"GAM Preference Landscapes -\", set_name),\n                      theme = theme(plot.title = element_text(size = 14, face = \"bold\")))\n    print(combined)\n\n    prefix <- gsub(\" \", \"_\", set_name)\n    ggsave(file.path(OUTPUT_DIR, paste0(prefix, \"_GAM_landscapes.png\")), combined,\n           width = 16, height = max(5, 5 * ceiling(length(gam_plots) / 3)),\n           dpi = 300, bg = \"white\", limitsize = FALSE)\n  }\n\n  all_results[[set_name]]$gam_plots <- gam_plots\n}\n\ncat(\"\\nGAM landscape generation complete.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Cell 11: Cosine Similarity — Likelihood Vectors vs Feature Directions\n\n# Find GAM argmax preference vector for each model score\ncalculate_likelihood_vectors_gam <- function(pca_results, model_scores,\n                                              grid_size = 60, k_basis = 12) {\n  data_clean <- pca_results$data %>%\n    dplyr::select(PC1, PC2, any_of(model_scores)) %>%\n    drop_na()\n\n  x_range <- range(data_clean$PC1)\n  y_range <- range(data_clean$PC2)\n  grid <- expand.grid(\n    PC1 = seq(x_range[1], x_range[2], length.out = grid_size),\n    PC2 = seq(y_range[1], y_range[2], length.out = grid_size)\n  )\n\n  results <- list()\n  for (sc in intersect(model_scores, names(data_clean))) {\n    form <- as.formula(paste0(\"`\", sc, \"` ~ s(PC1, PC2, k = \", k_basis, \")\"))\n    mod <- gam(form, data = data_clean, method = \"REML\")\n    pred <- predict(mod, newdata = grid)\n    best_idx <- which.max(pred)\n    best_point <- unlist(grid[best_idx, c(\"PC1\", \"PC2\")])\n    best_vec <- .unit_vec(best_point)\n\n    results[[sc]] <- list(\n      model = mod,\n      best_point = best_point,\n      best_vec = best_vec,\n      grid = grid,\n      predictions = pred,\n      Xp = predict(mod, newdata = grid, type = \"lpmatrix\"),\n      beta = coef(mod),\n      Vb = vcov(mod)\n    )\n    cat(sprintf(\"  %s: argmax at (%.3f, %.3f), unit vec (%.3f, %.3f)\\n\",\n                sc, best_point[1], best_point[2], best_vec[1], best_vec[2]))\n  }\n  results\n}\n\n# Composite feature loading vector (e.g., pH features)\ncalculate_feature_directions <- function(pca_results, feature_groups) {\n  loadings <- pca_results$loadings\n  directions <- list()\n\n  for (group_name in names(feature_groups)) {\n    feats <- intersect(feature_groups[[group_name]], loadings$Feature)\n    if (length(feats) == 0) next\n\n    sub_load <- loadings %>% filter(Feature %in% feats)\n    avg_vec <- c(PC1 = mean(sub_load$PC1), PC2 = mean(sub_load$PC2))\n    directions[[group_name]] <- .unit_vec(avg_vec)\n    cat(sprintf(\"  %s direction: PC1=%.3f, PC2=%.3f\\n\",\n                group_name, directions[[group_name]][1], directions[[group_name]][2]))\n  }\n  directions\n}\n\n# Cosine similarity matrix\ncompare_likelihood_to_features <- function(likelihood_results, feature_directions) {\n  results <- expand.grid(\n    model = names(likelihood_results),\n    feature_group = names(feature_directions),\n    stringsAsFactors = FALSE\n  )\n  results$cosine <- mapply(function(m, f) {\n    .cosine(likelihood_results[[m]]$best_vec, feature_directions[[f]])\n  }, results$model, results$feature_group)\n\n  results\n}\n\n# --- Run for each feature set ---\nfeature_group_defs <- list(\n  pH = pH_features,\n  compactness = c(\"rco\", \"structural_compactness\", \"centralization\"),\n  stability = c(\"instability_index\", \"gravy\"),\n  size = c(\"sequence_length\", \"mw_per_residue\"),\n  aromaticity = c(\"aromaticity\"),\n  secondary_structure = c(\"helix_sheet_contrast\", \"ordered_percent\")\n)\n\nfor (set_name in names(all_results)) {\n  cat(\"\\n=== COSINE SIMILARITY ANALYSIS:\", set_name, \"===\\n\")\n  pca_res <- all_results[[set_name]]$pca_results\n  scores_present <- intersect(available_scores, names(pca_res$data))\n\n  cat(\"Computing likelihood vectors...\\n\")\n  lik_vecs <- calculate_likelihood_vectors_gam(pca_res, scores_present)\n\n  cat(\"Computing feature directions...\\n\")\n  feat_dirs <- calculate_feature_directions(pca_res, feature_group_defs)\n\n  cosine_df <- compare_likelihood_to_features(lik_vecs, feat_dirs)\n\n  # Heatmap\n  p_heat <- ggplot(cosine_df, aes(x = feature_group, y = model, fill = cosine)) +\n    geom_tile() +\n    geom_text(aes(label = sprintf(\"%.2f\", cosine)), color = \"white\", fontface = \"bold\", size = 3) +\n    scale_fill_gradient2(limits = c(-1, 1), midpoint = 0, na.value = \"grey90\",\n                         low = \"#2166ac\", high = \"#b2182b\") +\n    labs(title = paste(\"Likelihood-Feature Alignment -\", set_name),\n         x = \"Feature Group\", y = \"Model\", fill = \"Cosine\") +\n    theme_minimal(base_size = 12) +\n    theme(axis.text.x = element_text(angle = 45, hjust = 1))\n  print(p_heat)\n\n  # Vector direction plot\n  vec_data <- bind_rows(\n    tibble(x = 0, y = 0,\n           xend = sapply(lik_vecs, function(v) v$best_vec[1]),\n           yend = sapply(lik_vecs, function(v) v$best_vec[2]),\n           name = names(lik_vecs), type = \"Model\"),\n    tibble(x = 0, y = 0,\n           xend = sapply(feat_dirs, `[`, 1),\n           yend = sapply(feat_dirs, `[`, 2),\n           name = names(feat_dirs), type = \"Feature\")\n  )\n\n  p_vec <- ggplot(vec_data, aes(x = x, y = y, xend = xend, yend = yend, color = name)) +\n    geom_segment(arrow = arrow(length = unit(0.2, \"inches\"), type = \"closed\"),\n                 linewidth = 1.2, alpha = 0.8) +\n    geom_text_repel(aes(x = xend, y = yend, label = name), size = 3) +\n    facet_wrap(~ type) +\n    labs(title = paste(\"Preference & Feature Vectors -\", set_name)) +\n    theme_minimal(base_size = 12) +\n    coord_fixed(xlim = c(-1.2, 1.2), ylim = c(-1.2, 1.2)) +\n    geom_hline(yintercept = 0, linetype = \"dashed\", color = \"grey70\") +\n    geom_vline(xintercept = 0, linetype = \"dashed\", color = \"grey70\") +\n    theme(legend.position = \"none\")\n  print(p_vec)\n\n  # Store & save\n  all_results[[set_name]]$cosine_analysis <- list(\n    likelihood_vectors = lik_vecs, feature_directions = feat_dirs,\n    cosine_df = cosine_df\n  )\n  prefix <- gsub(\" \", \"_\", set_name)\n  write_csv(cosine_df, file.path(OUTPUT_DIR, paste0(prefix, \"_cosine_similarities.csv\")))\n  ggsave(file.path(OUTPUT_DIR, paste0(prefix, \"_cosine_heatmap.png\")), p_heat,\n         width = 10, height = 6, dpi = 300, bg = \"white\")\n}\n\ncat(\"\\nCosine similarity analysis complete.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Cell 12: Fine-tuning Validation\n# Compares vanilla ProteinMPNN vs AlkalineMPNN using pH loading vector alignment\n\nvalidate_finetuning_effects <- function(pca_results, analysis_name = \"pH Features\",\n                                         vanilla_score = \"proteinmpnn_score\",\n                                         finetuned_score = \"AlkalineMPNN_score\",\n                                         n_bootstrap = 300, grid_size = 60, k_basis = 12) {\n  cat(\"\\n=== FINE-TUNING VALIDATION ===\", analysis_name, \"\\n\")\n\n  required_scores <- c(vanilla_score, finetuned_score)\n  avail <- required_scores[required_scores %in% colnames(pca_results$data)]\n  if (length(avail) < 2) {\n    cat(\"Missing required score columns:\", setdiff(required_scores, avail), \"\\n\")\n    return(NULL)\n  }\n\n  # pH loading vector\n  avail_pH <- intersect(pH_features, pca_results$loadings$Feature)\n  if (length(avail_pH) == 0) {\n    cat(\"No pH features in loadings; using all features\\n\")\n    load_use <- pca_results$loadings\n  } else {\n    load_use <- pca_results$loadings %>% filter(Feature %in% avail_pH)\n  }\n  ph_vector <- .unit_vec(c(PC1 = mean(load_use$PC1), PC2 = mean(load_use$PC2)))\n  cat(sprintf(\"pH loading vector: PC1=%.3f, PC2=%.3f\\n\", ph_vector[1], ph_vector[2]))\n\n  # Fit GAMs and find argmax\n  data_clean <- pca_results$data %>%\n    dplyr::select(PC1, PC2, all_of(avail)) %>% drop_na()\n  if (nrow(data_clean) < 200) cat(\"Note: <200 rows; CIs may be wide.\\n\")\n\n  grid <- expand.grid(\n    PC1 = seq(min(data_clean$PC1), max(data_clean$PC1), length.out = grid_size),\n    PC2 = seq(min(data_clean$PC2), max(data_clean$PC2), length.out = grid_size)\n  )\n\n  gam_fits <- list()\n  for (sc in avail) {\n    form <- as.formula(paste0(\"`\", sc, \"` ~ s(PC1, PC2, k = \", k_basis, \")\"))\n    mod <- gam(form, data = data_clean, method = \"REML\")\n    Xp <- predict(mod, newdata = grid, type = \"lpmatrix\")\n    beta <- coef(mod)\n    Vb <- vcov(mod)\n    pred_hat <- as.numeric(Xp %*% beta)\n    best_idx <- which.max(pred_hat)\n    best_vec <- .unit_vec(unlist(grid[best_idx, c(\"PC1\", \"PC2\")]))\n    gam_fits[[sc]] <- list(mod = mod, Xp = Xp, beta = beta, Vb = Vb, best_vec = best_vec)\n    cat(sprintf(\"  %s likelihood vector: PC1=%.3f, PC2=%.3f\\n\", sc, best_vec[1], best_vec[2]))\n  }\n\n  # Point estimate cosine similarities\n  cosine_sims <- sapply(gam_fits, function(fit) .cosine(ph_vector, fit$best_vec))\n  cat(\"\\nPoint estimate cosine similarities:\\n\")\n  for (sc in names(cosine_sims)) {\n    cat(sprintf(\"  %s: %.4f\\n\", sc, cosine_sims[sc]))\n  }\n\n  # Posterior simulation bootstrap\n  cat(\"\\nRunning posterior bootstrap (\", n_bootstrap, \"iterations)...\\n\")\n  boot_sims <- matrix(NA_real_, nrow = n_bootstrap, ncol = length(avail))\n  colnames(boot_sims) <- avail\n\n  for (b in seq_len(n_bootstrap)) {\n    for (sc in avail) {\n      fit <- gam_fits[[sc]]\n      beta_b <- MASS::mvrnorm(1, mu = fit$beta, Sigma = fit$Vb)\n      pred_b <- as.numeric(fit$Xp %*% beta_b)\n      idx_b <- which.max(pred_b)\n      v_b <- tryCatch(.unit_vec(unlist(grid[idx_b, c(\"PC1\", \"PC2\")])),\n                      error = function(e) c(NA, NA))\n      if (!any(is.na(v_b))) boot_sims[b, sc] <- .cosine(ph_vector, v_b)\n    }\n  }\n\n  # Confidence intervals\n  ci <- function(x) quantile(x, c(0.025, 0.975), na.rm = TRUE)\n  conf_intervals <- lapply(avail, function(sc) ci(boot_sims[, sc]))\n  names(conf_intervals) <- avail\n  for (sc in avail) {\n    cat(sprintf(\"  %s: 95%% CI [%.4f, %.4f]\\n\", sc,\n                conf_intervals[[sc]][1], conf_intervals[[sc]][2]))\n  }\n\n  # Significance test\n  diff_boot <- boot_sims[, finetuned_score] - boot_sims[, vanilla_score]\n  p_value <- 2 * min(mean(diff_boot <= 0, na.rm = TRUE), mean(diff_boot >= 0, na.rm = TRUE))\n  cat(sprintf(\"\\nFine-tuned vs Vanilla difference: %.4f, p-value: %.4f\\n\",\n              cosine_sims[finetuned_score] - cosine_sims[vanilla_score], p_value))\n\n  # Plots\n  sim_df <- data.frame(\n    Model = c(\"Vanilla ProteinMPNN\", \"AlkalineMPNN\"),\n    Cosine = c(cosine_sims[vanilla_score], cosine_sims[finetuned_score]),\n    CI_Lo = c(conf_intervals[[vanilla_score]][1], conf_intervals[[finetuned_score]][1]),\n    CI_Hi = c(conf_intervals[[vanilla_score]][2], conf_intervals[[finetuned_score]][2])\n  )\n\n  p_bar <- ggplot(sim_df, aes(x = Model, y = Cosine, fill = Model)) +\n    geom_col(alpha = 0.8, width = 0.6) +\n    geom_errorbar(aes(ymin = CI_Lo, ymax = CI_Hi), width = 0.2, linewidth = 1) +\n    scale_fill_manual(values = c(\"Vanilla ProteinMPNN\" = \"#e74c3c\", \"AlkalineMPNN\" = \"#3498db\")) +\n    labs(title = paste(\"pH Alignment:\", analysis_name),\n         subtitle = \"Cosine similarity with pH loading vector (GAM argmax)\",\n         y = \"Cosine Similarity\", x = NULL) +\n    theme_minimal(base_size = 12) +\n    theme(legend.position = \"none\", axis.text.x = element_text(angle = 30, hjust = 1))\n\n  var_x <- round(pca_results$explained_variance[1] * 100, 1)\n  var_y <- round(pca_results$explained_variance[2] * 100, 1)\n  vec_df <- data.frame(\n    x = 0, y = 0,\n    xend = c(gam_fits[[vanilla_score]]$best_vec[1],\n             gam_fits[[finetuned_score]]$best_vec[1], ph_vector[1]),\n    yend = c(gam_fits[[vanilla_score]]$best_vec[2],\n             gam_fits[[finetuned_score]]$best_vec[2], ph_vector[2]),\n    Label = c(\"Vanilla ProteinMPNN\", \"AlkalineMPNN\", \"pH Direction\")\n  )\n\n  p_vec <- ggplot(vec_df, aes(x = x, y = y, xend = xend, yend = yend, color = Label)) +\n    geom_segment(arrow = arrow(length = unit(0.3, \"inches\"), type = \"closed\"),\n                 linewidth = 1.5, alpha = 0.8) +\n    scale_color_manual(values = c(\"Vanilla ProteinMPNN\" = \"#e74c3c\",\n                                  \"AlkalineMPNN\" = \"#3498db\",\n                                  \"pH Direction\" = \"#9b59b6\")) +\n    labs(title = \"Likelihood vs pH Direction Vectors\",\n         x = paste0(\"PC1 (\", var_x, \"%)\"), y = paste0(\"PC2 (\", var_y, \"%)\")) +\n    theme_minimal(base_size = 12) +\n    theme(legend.position = \"bottom\", aspect.ratio = 1) +\n    coord_fixed(xlim = c(-1.2, 1.2), ylim = c(-1.2, 1.2)) +\n    geom_hline(yintercept = 0, linetype = \"dashed\", color = \"grey70\") +\n    geom_vline(xintercept = 0, linetype = \"dashed\", color = \"grey70\")\n\n  print(p_bar + p_vec)\n  ggsave(file.path(OUTPUT_DIR, paste0(gsub(\" \", \"_\", analysis_name), \"_finetuning_validation.png\")),\n         p_bar + p_vec, width = 14, height = 6, dpi = 300, bg = \"white\")\n\n  list(cosine_sims = cosine_sims, conf_intervals = conf_intervals,\n       p_value = p_value, ph_vector = ph_vector,\n       plots = list(bar = p_bar, vectors = p_vec))\n}\n\n# Run on pH features (primary) and mixed features\nft_results <- list()\nfor (set_name in c(\"pH Features\", \"Mixed Features\", \"Sequence Features\")) {\n  if (set_name %in% names(all_results)) {\n    ft_results[[set_name]] <- validate_finetuning_effects(\n      all_results[[set_name]]$pca_results, analysis_name = set_name\n    )\n  }\n}\ncat(\"\\nFine-tuning validation complete.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Cell 13: Load Design Data\n\ncat(\"=== LOADING DESIGN DATA ===\\n\")\n\n# Load and combine all per-model design CSVs\ndesigns_list <- list()\nfor (model_name in names(DESIGN_FILES)) {\n  fpath <- DESIGN_FILES[[model_name]]\n  if (!file.exists(fpath)) {\n    cat(\"  WARNING: File not found:\", fpath, \"\\n\")\n    next\n  }\n  d <- read_csv(fpath, show_col_types = FALSE)\n\n  # Standardize model column name\n  if (\"Model\" %in% names(d)) {\n    d <- d %>% rename(model = Model)\n  }\n  if (!\"model\" %in% names(d)) {\n    d$model <- model_name\n  }\n\n  designs_list[[model_name]] <- d\n  cat(sprintf(\"  %s: %d rows loaded\\n\", model_name, nrow(d)))\n}\n\nif (length(designs_list) == 0) {\n  cat(\"ERROR: No design files found. Skipping design analysis.\\n\")\n  cat(\"Please check DESIGN_DIR and DESIGN_FILES paths in Cell 3.\\n\")\n} else {\n  designs <- bind_rows(designs_list)\n  designs <- designs %>%\n    mutate(across(any_of(c(\"Entry\", \"design_id\", \"model\", \"domain\")), as.character)) %>%\n    mutate(across(ends_with(\"_shift\"), ~ .as_num(.x)))\n\n  cat(\"\\nCombined designs:\", nrow(designs), \"total rows\\n\")\n  cat(\"Models:\", paste(unique(designs$model), collapse = \", \"), \"\\n\")\n  cat(\"Unique proteins:\", n_distinct(designs$Entry), \"\\n\")\n\n  # Identify available shift columns\n  shift_cols <- grep(\"_shift$\", names(designs), value = TRUE)\n  cat(\"Shift columns:\", length(shift_cols), \"\\n\")\n}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Cell 14: Design Shift PCA — Project into WT Sequence-PCA Space (Figure 4)\n\nif (!exists(\"designs\")) {\n  cat(\"No design data loaded. Skipping.\\n\")\n} else {\n\ncat(\"=== DESIGN SHIFT PCA (Figure 4) ===\\n\")\n\n# 1) Fit WT sequence-PCA (6 features)\nwt_seq_pca <- perform_focused_pca(df, sequence_features)\nwt_seq_coords <- wt_seq_pca$data %>%\n  transmute(Entry, domain, PC1_wt = PC1, PC2_wt = PC2)\n\n# 2) Reconstruct absolute feature values for designs from WT + shift\nreconstruct_design_raw <- function(designs_df, wt_df, raw_features, key = \"Entry\") {\n  feats <- intersect(raw_features, names(wt_df))\n  if (!length(feats)) return(designs_df)\n\n  wt_base <- wt_df %>%\n    dplyr::select(all_of(c(key, feats))) %>%\n    rename_with(~ paste0(.x, \"__WT\"), .cols = -all_of(key))\n\n  d <- designs_df %>% left_join(wt_base, by = key)\n\n  for (f in feats) {\n    wt_col <- .as_num(d[[paste0(f, \"__WT\")]])\n    if (f == \"sequence_length\") { d[[f]] <- wt_col; next }\n\n    raw_col <- if (f %in% names(d)) .as_num(d[[f]]) else rep(NA_real_, nrow(d))\n    shift_nm <- paste0(f, \"_shift\")\n    shift_col <- if (shift_nm %in% names(d)) .as_num(d[[shift_nm]]) else rep(0, nrow(d))\n    d[[f]] <- dplyr::coalesce(raw_col, wt_col) + dplyr::coalesce(shift_col, 0)\n  }\n  d\n}\n\ndesigns_raw <- reconstruct_design_raw(designs, df, sequence_features)\n\n# 3) Project designs into WT PC space\nrecon_feats <- intersect(sequence_features, names(designs_raw))\nnum_ok <- designs_raw %>%\n  mutate(across(all_of(recon_feats), .as_num)) %>%\n  filter(if_all(all_of(recon_feats), ~ is.finite(.x)))\n\nX <- as.matrix(num_ok[, recon_feats, drop = FALSE])\nXs <- sweep(sweep(X, 2, wt_seq_pca$center[recon_feats], \"-\"),\n            2, wt_seq_pca$scale[recon_feats], \"/\")\nXs[!is.finite(Xs)] <- 0\nR <- wt_seq_pca$pca_object$rotation[recon_feats, 1:2, drop = FALSE]\nZ <- Xs %*% R\n\ndesigns_in_PCA <- num_ok %>%\n  mutate(PC1 = Z[, 1], PC2 = Z[, 2]) %>%\n  left_join(wt_seq_coords, by = c(\"Entry\", \"domain\")) %>%\n  filter(is.finite(PC1), is.finite(PC2))\n\ncat(\"Projected\", nrow(designs_in_PCA), \"designs into WT sequence-PCA space\\n\")\n\n# 4) Overall scatter by model\nvar_pc <- wt_seq_pca$explained_variance[1:2]\nlabs_xy <- pc_axis_labels(wt_seq_pca$explained_variance)\nratio_yx <- var_pc[2] / var_pc[1]\n\npal_model <- MODEL_COLORS[names(MODEL_COLORS) %in% unique(designs_in_PCA$model)]\n\np_all_models <- ggplot() +\n  geom_point(data = wt_seq_pca$data, aes(PC1, PC2), color = \"grey90\", alpha = 0.35, size = 0.8) +\n  geom_point(data = designs_in_PCA,\n             aes(PC1, PC2, color = model, fill = model),\n             size = 1.8, alpha = 0.7, shape = 21, stroke = 0.2) +\n  scale_color_manual(values = pal_model) +\n  scale_fill_manual(values = pal_model) +\n  labs(x = labs_xy$x, y = labs_xy$y,\n       title = \"Designs in WT sequence-PCA space\") +\n  theme_minimal(base_size = 12) +\n  theme(legend.position = \"bottom\",\n        panel.border = element_rect(color = \"grey60\", fill = NA)) +\n  coord_fixed(ratio = ratio_yx) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"grey70\") +\n  geom_vline(xintercept = 0, linetype = \"dashed\", color = \"grey70\")\nprint(p_all_models)\n\n# 5) Centroid shift arrows (Figure 4B left)\nper_entry_means <- designs_in_PCA %>%\n  group_by(Entry, model) %>%\n  summarise(PC1_mean = mean(PC1), PC2_mean = mean(PC2),\n            PC1_wt = first(PC1_wt), PC2_wt = first(PC2_wt), .groups = \"drop\") %>%\n  mutate(dPC1 = PC1_mean - PC1_wt, dPC2 = PC2_mean - PC2_wt)\n\nmodel_centroids <- per_entry_means %>%\n  group_by(model) %>%\n  summarise(dPC1 = mean(dPC1), dPC2 = mean(dPC2), .groups = \"drop\")\n\nwt_mean <- wt_seq_pca$data %>%\n  filter(Entry %in% unique(designs_in_PCA$Entry)) %>%\n  summarise(PC1 = mean(PC1), PC2 = mean(PC2))\n\np_arrows <- ggplot() +\n  geom_segment(data = model_centroids,\n               aes(x = wt_mean$PC1, y = wt_mean$PC2,\n                   xend = wt_mean$PC1 + dPC1, yend = wt_mean$PC2 + dPC2,\n                   color = model),\n               arrow = arrow(length = unit(0.2, \"inches\"), type = \"closed\"),\n               linewidth = 1.2) +\n  geom_point(data = wt_mean, aes(PC1, PC2), shape = 4, size = 4, stroke = 1.5) +\n  scale_color_manual(values = pal_model) +\n  labs(x = labs_xy$x, y = labs_xy$y,\n       title = \"Model centroid shifts (WT mean to design mean)\") +\n  theme_minimal(base_size = 12) +\n  theme(legend.position = \"bottom\",\n        panel.border = element_rect(color = \"grey60\", fill = NA)) +\n  coord_fixed(ratio = ratio_yx) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"grey70\") +\n  geom_vline(xintercept = 0, linetype = \"dashed\", color = \"grey70\")\nprint(p_arrows)\n\n# 6) Per-protein scatter examples (4 representative proteins)\nexample_entries <- designs_in_PCA %>%\n  distinct(Entry) %>%\n  slice_head(n = 4) %>%\n  pull(Entry)\n\nper_prot_plots <- list()\nfor (eid in example_entries) {\n  d_one <- designs_in_PCA %>% filter(Entry == eid)\n  wt_pt <- d_one %>% distinct(PC1_wt, PC2_wt)\n\n  per_prot_plots[[eid]] <- ggplot() +\n    geom_point(data = designs_in_PCA, aes(PC1, PC2), color = \"grey85\", alpha = 0.3, size = 0.8) +\n    geom_point(data = d_one, aes(PC1, PC2, color = model), size = 2, alpha = 0.85) +\n    geom_point(data = wt_pt, aes(PC1_wt, PC2_wt), shape = 4, size = 3, stroke = 1) +\n    scale_color_manual(values = pal_model) +\n    labs(title = eid, x = labs_xy$x, y = labs_xy$y) +\n    theme_minimal(base_size = 10) +\n    theme(legend.position = \"none\") +\n    coord_fixed(ratio = ratio_yx)\n}\n\nif (length(per_prot_plots) > 0) {\n  print(wrap_plots(per_prot_plots, ncol = 2) +\n    plot_annotation(title = \"Per-protein designs in WT sequence-PCA\",\n                    subtitle = \"X = wild-type position\"))\n}\n\n# Save\nggsave(file.path(OUTPUT_DIR, \"design_shift_overall_by_model.png\"), p_all_models,\n       width = 9, height = 7, dpi = 300, bg = \"white\")\nggsave(file.path(OUTPUT_DIR, \"design_shift_centroid_arrows.png\"), p_arrows,\n       width = 8, height = 7, dpi = 300, bg = \"white\")\n\ncat(\"Design shift PCA visualization complete.\\n\")\n}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Cell 15: Design Shift — Quantitative Analysis\n\nif (!exists(\"designs_in_PCA\")) {\n  cat(\"No design PCA data. Skipping.\\n\")\n} else {\n\ncat(\"=== QUANTITATIVE DESIGN SHIFT ANALYSIS ===\\n\")\n\n# 1) Per-model centroids and one-sample t-tests\nper_model_tests <- per_entry_means %>%\n  group_by(model) %>%\n  summarise(\n    n = n(),\n    mean_dPC1 = mean(dPC1), sd_dPC1 = sd(dPC1),\n    mean_dPC2 = mean(dPC2), sd_dPC2 = sd(dPC2),\n    t_PC1 = tryCatch(t.test(dPC1)$statistic, error = function(e) NA),\n    p_PC1 = tryCatch(t.test(dPC1)$p.value, error = function(e) NA),\n    t_PC2 = tryCatch(t.test(dPC2)$statistic, error = function(e) NA),\n    p_PC2 = tryCatch(t.test(dPC2)$p.value, error = function(e) NA),\n    d_PC1 = mean(dPC1) / sd(dPC1),  # Cohen's d\n    d_PC2 = mean(dPC2) / sd(dPC2),\n    .groups = \"drop\"\n  )\n\ncat(\"\\nPer-model centroid shift tests:\\n\")\nprint(per_model_tests)\nwrite_csv(per_model_tests, file.path(OUTPUT_DIR, \"design_shift_model_tests.csv\"))\n\n# 2) Back-project centroid shifts into feature space\nR12 <- wt_seq_pca$pca_object$rotation[wt_seq_pca$features, 1:2, drop = FALSE]\nscales <- wt_seq_pca$scale[wt_seq_pca$features]\n\nmodel_centroids_pc <- per_entry_means %>%\n  group_by(model) %>%\n  summarise(dPC1_centroid = mean(dPC1), dPC2_centroid = mean(dPC2), .groups = \"drop\")\n\nmodel_feature_deltas <- model_centroids_pc %>%\n  rowwise() %>%\n  mutate(bp = list({\n    dz <- as.numeric(R12 %*% c(dPC1_centroid, dPC2_centroid))\n    tibble(feature = wt_seq_pca$features, dz = dz, d_raw = dz * scales[feature])\n  })) %>%\n  unnest(bp) %>%\n  ungroup() %>%\n  arrange(model, desc(abs(dz)))\n\ncat(\"\\nBack-projected feature deltas (top per model):\\n\")\nmodel_feature_deltas %>%\n  group_by(model) %>%\n  slice_head(n = 3) %>%\n  print(n = 20)\nwrite_csv(model_feature_deltas, file.path(OUTPUT_DIR, \"design_shift_feature_deltas.csv\"))\n\n# 3) Per-feature t-tests with BH FDR correction\nshift_feat_cols <- paste0(sequence_features, \"_shift\")\navail_shift <- intersect(shift_feat_cols, names(designs))\n\nif (length(avail_shift) > 0) {\n  feat_tests <- designs %>%\n    dplyr::select(model, all_of(avail_shift)) %>%\n    pivot_longer(cols = all_of(avail_shift), names_to = \"feature\", values_to = \"shift\") %>%\n    group_by(model, feature) %>%\n    summarise(\n      n = sum(!is.na(shift)),\n      mean_shift = mean(shift, na.rm = TRUE),\n      sd_shift = sd(shift, na.rm = TRUE),\n      t_stat = tryCatch(t.test(shift)$statistic, error = function(e) NA),\n      p_value = tryCatch(t.test(shift)$p.value, error = function(e) NA),\n      cohens_d = mean(shift, na.rm = TRUE) / sd(shift, na.rm = TRUE),\n      .groups = \"drop\"\n    ) %>%\n    group_by(model) %>%\n    mutate(p_adj = p.adjust(p_value, method = \"BH\")) %>%\n    ungroup()\n\n  cat(\"\\nPer-feature shift tests (BH-corrected):\\n\")\n  feat_tests %>% filter(p_adj < 0.05) %>% print(n = 30)\n  write_csv(feat_tests, file.path(OUTPUT_DIR, \"design_shift_feature_tests.csv\"))\n}\n\n# 4) Cosine similarity between models (per-protein WT->design vectors)\ncompute_model_cosine_similarity <- function(pca_df) {\n  mean_coords <- pca_df %>%\n    group_by(Entry, model) %>%\n    summarise(PC1_mean = mean(PC1), PC2_mean = mean(PC2),\n              PC1_wt = first(PC1_wt), PC2_wt = first(PC2_wt), .groups = \"drop\") %>%\n    mutate(dPC1 = PC1_mean - PC1_wt, dPC2 = PC2_mean - PC2_wt)\n\n  keep <- mean_coords %>% count(Entry) %>% filter(n >= 2) %>% pull(Entry)\n  mc <- mean_coords %>% filter(Entry %in% keep)\n\n  pairs <- mc %>%\n    group_by(Entry) %>%\n    summarise(d = list(expand_grid(model_i = model, model_j = model)), .groups = \"drop\") %>%\n    unnest(d) %>%\n    filter(model_i < model_j) %>%\n    left_join(mc %>% dplyr::select(Entry, model, dPC1, dPC2), by = c(\"Entry\", \"model_i\" = \"model\")) %>%\n    rename(dPC1_i = dPC1, dPC2_i = dPC2) %>%\n    left_join(mc %>% dplyr::select(Entry, model, dPC1, dPC2), by = c(\"Entry\", \"model_j\" = \"model\")) %>%\n    rename(dPC1_j = dPC1, dPC2_j = dPC2) %>%\n    mutate(cosine = .cosine2(dPC1_i, dPC2_i, dPC1_j, dPC2_j))\n\n  summary <- pairs %>%\n    group_by(model_i, model_j) %>%\n    summarise(n = sum(!is.na(cosine)),\n              mean_cos = mean(cosine, na.rm = TRUE),\n              sd_cos = sd(cosine, na.rm = TRUE), .groups = \"drop\")\n\n  list(pairs = pairs, summary = summary)\n}\n\ncos_result <- compute_model_cosine_similarity(designs_in_PCA)\ncat(\"\\nPairwise model cosine similarity (mean across proteins):\\n\")\nprint(cos_result$summary)\n\n# Cosine heatmap\nrev_df <- cos_result$summary %>% rename(model_j = model_i, model_i = model_j)\ndiag_models <- unique(c(cos_result$summary$model_i, cos_result$summary$model_j))\nsq <- bind_rows(cos_result$summary, rev_df) %>%\n  bind_rows(tibble(model_i = diag_models, model_j = diag_models,\n                   n = NA_integer_, mean_cos = 1, sd_cos = NA_real_)) %>%\n  distinct(model_i, model_j, .keep_all = TRUE)\n\np_cos_heat <- ggplot(sq, aes(model_i, model_j, fill = mean_cos)) +\n  geom_tile() +\n  geom_text(aes(label = ifelse(is.na(n), \"\", sprintf(\"%.2f\", mean_cos))),\n            color = \"white\", fontface = \"bold\", size = 3.5) +\n  scale_fill_gradient2(limits = c(-1, 1), midpoint = 0) +\n  labs(title = \"Mean cosine similarity between model shift vectors\",\n       x = NULL, y = NULL, fill = \"Cosine\") +\n  theme_minimal(base_size = 12) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n  coord_equal()\nprint(p_cos_heat)\n\nwrite_csv(cos_result$summary, file.path(OUTPUT_DIR, \"model_cosine_summary.csv\"))\nwrite_csv(cos_result$pairs, file.path(OUTPUT_DIR, \"model_cosine_per_protein.csv\"))\nggsave(file.path(OUTPUT_DIR, \"model_cosine_heatmap.png\"), p_cos_heat,\n       width = 7, height = 6, dpi = 300, bg = \"white\")\n\ncat(\"\\nQuantitative design shift analysis complete.\\n\")\n}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Cell 16: Save & Export\n\ncat(\"=== SAVING ALL OUTPUTS ===\\n\")\n\n# List all output files\noutput_files <- list.files(OUTPUT_DIR, recursive = TRUE, full.names = TRUE)\ncat(\"Output files generated:\\n\")\nfor (f in output_files) {\n  size_kb <- round(file.info(f)$size / 1024, 1)\n  cat(sprintf(\"  %s (%s KB)\\n\", basename(f), size_kb))\n}\n\n# Zip for download (useful on Colab)\nzip_path <- paste0(OUTPUT_DIR, \".zip\")\nif (length(output_files) > 0) {\n  zip(zip_path, OUTPUT_DIR)\n  cat(\"\\nZipped to:\", zip_path, \"\\n\")\n}\n\ncat(\"\\n=== ANALYSIS COMPLETE ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}